[
["index.html", "Geocomputation with R Welcome Development Reproducibility", " Geocomputation with R Robin Lovelace Jakub Nowosad 2017-08-11 Welcome Welcome to geocompr, the website of our forthcoming book with CRC Press. Development Inspired by the bookdown R package we are developing this book in the open. We decided to make the book open source to encourage contributions, ensure reproducibility and provide access to the material as it evolves. We’re developing the book in 3 main phases. We’re in phase 1 and focussed on the first 5 main chapters, which we aim to be complete by September. Drafts of other chapters will be added to this website as the project progresses. The latest version is hosted at robinlovelace.net/geocompr. This website is kept up-to-date thanks to Travis, a continuous integration (CI) service. Travis automatically rebuilds the book and finds bugs by reporting the ‘build status’ after every update. Currently the build is: The version of the book you are reading now was built on 2017-08-11 and was built on Travis. bookdown makes editing a book as easy as editing a wiki. To do so, just click on the ‘edit me’ icon highlighted in the image below. Which-ever chapter you are looking at, this will take you to the source R Markdown file hosted on GitHub. If you have a GitHub account, you’ll be able to make changes there and submit a pull request. If you do not, it’s time to sign-up! To raise an issue about the book’s content (e.g. code not running) or make a feature request, check-out the issue tracker. Reproducibility To reproduce the book, you need a recent version of R and up-to-date packages. The following code should install the required packages: if(!require(devtools)) { install.packages(&quot;devtools&quot;) } devtools::install_github(&quot;robinlovelace/geocompr&quot;) To build the book locally, clone or download the repo and run the following line from the project’s root directory: bookdown::render_book(&quot;index.Rmd&quot;) # to build the book browseURL(&quot;_book/index.html&quot;) # to view it For further details see the book’s GitHub page at Robinlovelace/geocompr. "],
["intro.html", "1 Introduction 1.1 What is geocomputation? 1.2 Why Geocomputation with R? 1.3 Software for geocomputation 1.4 R’s spatial ecosystem 1.5 R’s spatial history 1.6 Exercises", " 1 Introduction This book is about harnessing the power of modern computers to do things with geographic data. It teaches a range of spatial skills, including: reading, writing and manipulating geographic data; making static and interactive maps; and modeling geographic phenomena. By demonstrating how various spatial operations can be linked, in the reproducible ‘code chunks’ that intersperse the prose, the book also shows how these skills support a transparent and thus scientific workflow. Learning how to use existing tools is exciting. However, it is even more liberating, if you can create new tools yourself. By the end of the book you should be well-equipped enough to create new tools in the form of shareable R functions. Over the last few decades a huge amount of work has gone into developing Free and Open Source Software for Geospatial Applications (FOSS4G). This means that spatial data analysis is no longer the preserve of those who can afford expensive programs, and the hardware to run them. Anyone can now download high performance spatial libraries on their computer. However, despite the growth of geospatial software that is open source, much of it remains inaccessible to many potential users due to the required expert knowledge to handle it. A major aim of this book is to make geographical data analysis more accessible. R is a flexible language that allows access to many spatial software libraries (see section 1.2). Before going into the details of the software, however, it is worth taking a step back and thinking about what we mean by geocomputation. 1.1 What is geocomputation? Geocomputation is a relatively young field with its [~30 year history dating back to the first conference on the subject in 19961. What distinguishes geocomputation from the older quantitative geography, is its emphasis on “creative and experimental” GIS applications (Longley et al. 1998). Additionally, it is also about developing new, research-driven methods (Openshaw and Abrahart 2000): GeoComputation is about using the various different types of geodata and about developing relevant geo-tools within the overall context of a ‘scientific’ approach. But geocomputation and this book teach more than just methods and code: they are about doing “practical work that is beneficial or useful” (Openshaw and Abrahart 2000). Of course, reading this book will give you a solid knowledge of geocomputational methods, and how to use them via the reproducible examples implemented in the code chunks in each chapter. But there is much more. This book aims to teach how to do geocomputation rather than just to think about it. Hence, you should be also able to apply the learned methods and mastered skills to real-world data, i.e. your data. Moreover, throughout the book we encourage you to make geographic research more reproducible, scientific and socially beneficial. Please note that this book is also part of the movement towards Geographical Information Science (GDS) which supports the above mentioned concepts. GSD also differs from GIS in several ways, some of which are outlined in Table 1.1. Table 1.1: Differences in emphasis between the fields of Geographic Information Systems (GIS) and Geographic Data Science (GDS). Attribute GIS GDS Home disciplines Geography Geography, Computing, Statistics Software focus Graphical User Interface Code Reproduciblility Minimal Maximal While embracing recent developments in the field, we also wanted to pay respects to the wider field of Geography, with its 2000 history (Roller 2010), and the narrower field of Geographic Information System (GIS) (Neteler and Mitasova 2008). Geography has played an important role in explaining and influencing humanity’s relationship with the natural world (just think of Humboldt’s travels to South America which laid the foundations, among others, for physical and plant geography; Wulf 2015) and this book aims to contribute to this so-called ‘Geographic tradition’ (Livingstone 1992). GIS has become almost synonymous with handling spatial data on a computer, and provides a basis for excellent open source tools which can be accessed from R, as we will see in Chapter 13. The book’s links to older disciplines were reflected in suggested titles for the book: Geography with R and R for GIS. Each has advantages. The former conveys the message that it comprises much more than just spatial data: non-spatial attribute data are inevitably interwoven with geometry data, and Geography is about more than where something is on the map. The latter communicates that this is a book about using R as a GIS, to perform spatial operations on geographic data (Bivand, Pebesma, and Gómez-Rubio 2013). However, the term GIS conveys some connotations (see Table 1.1) which simply fail to communicate one of R’s greatest strengths: its console-based ability to seamlessly switch between geographic and non-geographic data processing, modeling and visualization tasks. By contrast, the term geocomputation implies reproducible and creative programming. Of course, (geocomputational) algorithms are powerful tools that can become highly complex. However, all algorithms are composed of smaller parts. By teaching you its foundations and underlying structure, we aim to empower you to create your own innovative solutions to geographic data problems. 1.2 Why Geocomputation with R? Early geographers used a variety of tools including rulers, compasses and sextants to advance knowledge about the world. However, until John Harrison invented the marine chronometer in the 18th century it had been impossible to determine the exact longitude at sea (‘the longitude problem’). Prior to his invention ships followed for centuries a line of constant latitude making each journey much longer, more expensive and often also more dangerous. Nowadays this seems unimaginable with every smartphone having a GPS receiver at its disposal. And there are a multitude of other sensors measuring the world in real-time (satellites, radar, autonomous cars, citizens, etc.). For instance, an autonomous car could create 100 GB or more per day (see e.g., this article). Equally, earth observation data (satellite imagery) has become so big that it is impossible to analyze the corresponding data with a single computer (see http://r-spatial.org/2016/11/29/openeo.html). Hence, we need computational power, software and related tools to handle and extract the most interesting patterns of this ever-increasing amount of (geo-)data. (Geo-)Databases help with data management, storing and querying such large amounts of data. Through interfaces we can access subsets of these data for further analysis, information extraction and visualization. In this book we treat R as a ‘tool for the trade’ for the latter. R is a multi-platform, open source language for statistical computing and graphics (https://www.r-project.org/). With a wide range of packages R also supports advanced geospatial statistics, modeling and visualisation.2. At its core R is an object-oriented, functional programming language (Wickham 2014), and was specifically designed as an interactive interface to other software (Chambers 2016). The latter also includes many ‘bridges’ to a treasure trove of GIS software, geolibraries and functions. It is thus ideal for quickly creating ‘geo-tools’, without needing to master lower level languages (compared to R) such as C, FORTRAN and Java (see section 1.3). This can feel like breaking free from the metaphorical ‘glass ceiling’ imposed by GUI-based proprietary geographic information systems (see Table 1.1 for a definition of GUI). What is more, advanced users might even extend R with the power of other languages (e.g., C++ through Rcpp or Python through reticulate; see also section 1.3). An example showing R’s flexibility with regard to geographic software development is its support for generating interactive maps thanks to leaflet (Cheng, Karambelkar, and Xie 2017). The packages tmap and mapview (Tennekes 2017; Appelhans et al. 2017) are built on and extend leaflet. These packages help overcome the criticism that R has “limited interactive [plotting] facilities” (Bivand, Pebesma, and Gómez-Rubio 2013). The code below illustrates this by generating Figure 1.1. library(leaflet) popup = c(&quot;Robin&quot;, &quot;Jakub&quot;) leaflet() %&gt;% addProviderTiles(&quot;NASAGIBS.ViirsEarthAtNight2012&quot;) %&gt;% addAwesomeMarkers(lng = c(-3, 23), lat = c(52, 53), popup = popup) Figure 1.1: World at night imagery from NASA overlaid by the authors’ approximate home locations to illustrate interactive mapping with R. It would have been difficult to produce Figure 1.1 using R a few years ago, let alone embed the results in an interactive html page (the interactive version can be viewed at robinlovelace.net/geocompr). This illustrates R’s flexibility and how, thanks to developments such as knitr and leaflet, it can be used as an interface to other software, a theme that will recur throughout this book. The use of R code, therefore, enables teaching geocomputation with reference to reproducible examples such as that provided in 1.1 rather than abstract concepts. 1.3 Software for geocomputation This section introduces commonly used languages for geocomputation: C++, Java and Python with a focus on the latter. A natural choice for geocomputation would be C++ since most major GIS packages (e.g., GEOS, GDAL, QGIS, GRASS, SAGA, and even ArcGIS) often rely in great parts on it. This is because well-written C++ can be blazzingly fast, which makes it a good choice for performance-critical applications such as the processing of large spatial data. Usually, people find it harder to learn than Python or R. It is also likely that you have to invest a lot of time to code things that are readily available in R. Therefore, we would recommend to learn R, and subsequently to learn C++ through Rcpp if a need for performance optimization arises. Subsequently, you could even implement geoalgorithms you are missing from the most common desktop GIS with the help of Rcpp3–&gt; ]. Java is another important (and versatile) language in GIScience. For example, the open-source vector GIS OpenJump is written in the Java programming language. Java’s object-oriented syntax is similar to C++, but much simpler. Still, it is rather unforgiving regarding class, object and variable declarations forcing you to think about a well-designed programming structure. This is especially useful in large projects with thousands of lines of codes placed in numerous files. Following the write once, run anywhere principle, Java is platform-independent (which is unusual for a compiled programming language). There are many open source add-on libraries available for Java, including GeoTools. Overall, Java has an excellent performance on large-scale systems making it a suitable candidate as the basis for a desktop GIS. However, it is probably less suitable for statistical modeling and visualization compared to Python or R. Finally, there is Python for geoprocessing. We will dwell a little bit more on Python since many people believe that R and Python are battling for supremacy in the field of data science. This is accompanied by a partly offensive and often rather subjective discussion on what to learn or what to use. We believe that both languages have their merits, and in the end it is about doing geocomputation and communicating the corresponding results regardless of the chosen software. Moreover, both languages are object-oriented, and have lots of further things in common. Learning one language should give you a headstart when choosing to learn the other as well. R’s major advantage is that statisticians wrote hundreds of statistical packages (unmatched by Python) explicitly for other statisticians. By contrast, Python’s major advantage is that it is (unlike R) a multi-purpose language thereby bringing together people from diverse fields which also explains Python’s bigger user base compared to R’s. So if you like Python better or you think it better suits your needs (for example because you are also interested in web and GUI development), go for it. In fact, we often advise our students to start with Python just because the major GIS software packages provide Python libraries that lets the user access its geoalgorithms from the Python command line4. However, when the teaching moves on to statistical geoprocessing and spatial predictive modeling we guide them towards R where they can take advantage of the concepts already learned through Python. Nevertheless, you can also use Python for the most common statistical learning techniques (though R tends to be more on the bleeding edge regarding new statistical development including those in the geostatistical community). In addition, Python also offers excellent support for spatial data analysis and manipulation (see packages osgeo, Shapely, NumPy, osgeo, PyGeoProcessing). We refer you to (???) for an introduction to geoprocessing with Python. 1.4 R’s spatial ecosystem Before cracking-on with the action, a few introductory remarks are needed to explain the approach taken here and provide context. sf + raster/stars + leaflet/mapview (the recent state of spatial R); the history of R spatial is way longer --> There are many ways to handle spatial data in R, with dozens of packages in the area.5 In this book we endeavor to teach the state-of-the-art in the field whilst ensuring that the methods are future-proof. Like many areas of software development, R’s spatial ecosystem is rapidly evolving. Because R is open source, these developments can easily build on previous work, by ‘standing on the shoulders of giants’, as Isaac Newton put it in 1675. This approach is advantageous because it encourages collaboration and avoids ‘reinventing the wheel’. The package sf (covered in Chapter 2), for example, builds on its predecessor sp. A surge in development time (and interest) in ‘R-Geo’ has followed the award of a grant by the R Consortium for the development of support for Simple Features, an open-source standard and model to store and access vector geometries. This resulted in the sf package (covered in 2.1.1). Multiple places reflect the immense interest in sf. This is especially true for the R-sig-Geo Archives, a long-standing open access email list containing much R-spatial wisdom accumulated over the years. Many posts on the list now discuss sf and related packages, suggesting that R’s spatial software developers are using the package and, therefore, it is here to stay. We even propose that the release of sf heralds a new era for spatial data analysis and geocomputation in R. This era6 clearly has the wind in its sails, and is set to dominate future developments in R’s spatial ecosystem for years to come. So time invested in learning the ‘new ways’ of handling spatial data and, hopefully, reading this book, is well spent! Figure 1.2: The popularity of spatial packages in R. The y-axis shows the average number of downloads, within a 30-day rolling window, of R’s top 5 spatial packages, defined as those with the highest number of downloads within the last 30 days. It is noteworthy that shifts in the wider R community, as exemplified by the data processing package dplyr (released in 2014) influenced shifts in R’s spatial ecosystem. Alongside other packages that have a shared style and emphasis on ‘tidy data’ (including e.g., ggplot2), dplyr was placed in the tidyverse ‘metapackage’ in late 2016. The tidyverse approach, with its focus on long-form data and fast, intuitively named functions, has become immensely popular. This has led to a demand for ‘tidy spatial data’ which has been partly met by sf and other approaches such as the GitHub package tabularaster. An obvious feature of the tidyverse is the tendency for packages to work in harmony. Although an equivalent geoverse is presently missing, there is an on-going discussion of harmonizing R’s many spatial packages.7 and a growing number of actively developed packages which are designed to work in harmony with sf (Table 1.2) and Table 1.2: The top 5 most downloaded packages that depend on sf, in terms of average number of downloads per day over the previous month. As of 2017-08-09 there are 23 packages which import sf. package Downloads plotly 1702 leaflet 485 mapview 202 geojsonio 174 tigris 95 1.5 R’s spatial history There are many benefits of using recent packages such as sf, with the caveat that they are generally less stable than mature packages such as its predecessor, the sp-package. The saying “if you live on the cutting edge you risk getting hurt” captures this well, meaning that older packages may be more appropriate for applications requiring stability and backwards-compatibility with other mature packages. Another reason for knowing about the history of geocomputation with R is that there is a wealth of functions, use-cases and teaching material written using older packages in R’s spatial ecosystem, which can still be useful today if you know where to look. The beginnings of spatial capabilities in R are closely connected with its predecessor - the S language (Bivand and Gebhardt 2000). The 1990s saw the development of numerous S scripts and a handful of packages for spatial statistics. Some of these, including spatial, sgeostat and splancs, eventually became R packages (Rowlingson and Diggle 1993; Rowlingson and Diggle 2017; Venables and Ripley 2002; University and Gebhardt 2016). Volume 1/2 of R News (the predecessor of The R Journal) contained an overview of spatial statistical software in R at the time, much of which was based on previous code written for S/S-PLUS (Ripley 2001). This overview described packages for spatial smoothing and interpolation (e.g., akima, spatial, sgeostat and geoR) and point pattern analysis (splancs and spatstat; Akima and Gebhardt 2016; Rowlingson and Diggle 2017; Jr and Diggle 2016). While all these are still available on CRAN, spatstat stands out among them, as it remains dominant in the field of spatial point pattern analysis (Baddeley, Rubak, and Turner 2015). The following R News issue (Volume 1/3) put spatial packages in the spotlight again, with an introduction to splancs and a commentary on future prospects regarding spatial statistics (Bivand 2001). Additionally, the issue introduced two packages for testing spatial autocorrelation that eventually became part of spdep (Bivand 2017). Notably, the commentary mentions the need for standardization of spatial interfaces, efficient mechanisms for exchanging data with GIS, and handling of spatial metadata such as coordinate reference systems (CRS). maptools (written by Nicholas Lewin-Koh; Bivand and Lewin-Koh 2017) is another important package from this time. Initially, maptools just contained a wrapper around shapelib, and permitted the reading of ESRI Shapefiles into geometry nested lists. The corresponding and nowadays obsolete S3 class called “Map” stored this list alongside an attribute data frame. The work on the “Map” class representation was nevertheless important since it directly fed into sp prior to its publication on CRAN. In 2003, Hornik et al. (2003) published an extended review of spatial packages. Around this time the development of R’s spatial capabilities increasingly supported interfaces to external libraries, especially to GDAL and PROJ.4. These interfaces facilitated geographic data I/O (covered in chapter 5) and CRS transformations, respectively. Bivand (2003) proposed a spatial data class system, including support for points, lines, polygons and grids based on GDAL’s support for a wide range of spatial data formats. All these ideas contributed to the packages rgdal and sp, which became the foundational packages for spatial data analysis with R (Bivand, Pebesma, and Gómez-Rubio 2013). Naturally, rgdal, first released in 2003, greatly extended R’s spatial capabilities in terms of access to previously unavailable spatial data formats. Initially, only raster drivers were supported, based on Tim Keitt’s GDAL bindings for R. Importantly, rgdal enabled storing information about coordinate reference system, and allowed map projections and datum transformations. Many of these additional capabilities were developed by Barry Rowlingson, and folded into the rgdal codebase (because the same underlying external dependencies were needed). sp, released in 2005, overcame R’s inability to distinguish spatial and non-spatial objects. It grew from a workshop before, and a session at the 2003 R conference in Vienna, gathering input from most interested package developers. At the same time, sourceforge was chosen for development collaboration (migrated to R-Forge five years later) and the R-sig-geo mailing list was started. Prior to 2005, spatial coordinates were generally treated as any other number. This changed with sp as it provided generic classes and methods for spatial data. The sophisticated class system supported points, lines, polygons and grids, with and without attribute data. Making use of the S4 class system, sp stores each piece of ‘spatially specific’ information (such as bounding box, coordinate reference system, attribute table) in slots, which are accessible via the @ symbol. For instance, sp-classes store attribute data in the data slot as a data.frame. This enables non-spatial data operations to work alongside spatial operations (covered in chapters 3 and 4, respectively). Additionally, sp implemented generic methods for spatial data types for well-known functions such as summary() and plot() . In the following, sp classes rapidly became the go-to standard for spatial data in R, resulting in a proliferation of packages that depended on it from around 20 in 2008 and over 100 in 2013 (Bivand, Pebesma, and Gómez-Rubio 2013). Now more than 200 packages rely on sp, making it an important part of the R ecosystem. Prominent R packages using sp include: gstat, for spatial and spatio-temporal geostatistics; geosphere, for spherical trigonometry; and adehabitat used for the analysis of habitat selection by animals (Pebesma and Graeler 2017; Calenge 2006; Hijmans 2016a). While rgdal and sp solved many spatial issues, R was still lacking geometry calculation abilities. Therefore, Colin Rundel started to develop a package that interfaces GEOS, an open-source geometry library, during a Google Summer of Coding project in 2010. The resulting rgeos package (first released in 2010; Bivand and Rundel 2017) brought geometry calculations to R by allowing functions and operators from the GEOS library to manipulate sp objects. Another limitation of sp was its limited support of raster data. The raster-package (first released in 2010; Hijmans 2016b) overcame this by providing a raster class and functions for creating, reading and writing raster data. A key feature of raster is its ability to work with data sets that are too large to fit into the main memory (RAM), thereby overcoming one of R’s major limitations when working on raster data.8 In parallel with or partly even preceding the development of spatial classes and methods came the support for R as an interface to dedicated GIS software. The GRASS package (Bivand 2000) and follow-on packages spgrass6 and rgrass7 (for GRASS GIS 6 and 7, respectively) were prominent examples in this direction (R. Bivand 2016b; R. Bivand 2016a). Other examples of bridges between R and GIS include RSAGA (Brenning and Bangs 2016, first published in 2008), ArcGIS (Brenning 2012, first published in 2008), and RQGIS (Muenchow and Schratz 2017, first published in 2016). Map making was not a focus of R’s early spatial capabilities. But soon sp provided methods for advanced map making using both the base and lattice plotting system. Despite this, a demand for the layered grammar of graphics was growing especially after the release of ggplot2 in 2007. ggmap extended ggplot2’s spatial capabilities (Kahle and Wickham 2013). However, its main purpose was the easy access of several APIs to automatically download map tiles (among others, Google Maps and OpenStreetmap) and subsequent plotting of these as a basemap. Though ggmap facilitated map-making with ggplot2, one main limitation remained. To make spatial data work with the ggplot2 system, one needed to fortify spatial objects. Basically, this means, you need to combine the coordinates and attribute slots of a spatial class object into one data frame. While this works well in the case of points, it duplicates the same information over and over again in the case of polygons, since each coordinate (vertex) of a polygon receives the attribute data of the polygon. This is especially disadvantageous if you need to deal with tens of thousands of polygons. With the introduction of simple features to R this limitation disappears, and it seems likely that this will make ggplot2 the standard tool for the visualization of vector data. This might be different regarding the visualization of raster data. Raster visualization methods received a boost with the release of rasterVis (Lamigueiro 2014) which builds on top of the lattice system. More recently, new packages aim at easing the creation of complex, high-quality maps with minimal code. The tmap package (released in 2014) might serve as an archetype for this kind of development (Tennekes 2017). It facilitates the user-friendly creation of thematic maps with an intuitive command-line interface (see also mapmisc) . tmap is a sophisticated yet user friendly mapping package which works in harmony with the leaflet package (released in 2015) for interactive map making (Cheng, Karambelkar, and Xie 2017). Similarly, the mapview package builds also on top of leaflet (Appelhans et al. 2017) for interactive mapping based on sp or sf objects. mapview allows the access of a wide range of background maps, scale bars and more. However, it is noteworthy that among all the recent developments described above, the support for simple features (sf; Pebesma 2017) has been without doubt the most important evolution in R’s spatial ecosystem. Naturally, this is the reason why we will describe sf in detail in Chapter 2. 1.6 Exercises Think about the terms ‘GIS’, ‘GDS’ and ‘Geocomputation’ described above. Which is your favorite, and why? Provide three reasons for using a scriptable language such as R for geocomputation instead of using an established GIS program such as QGIS. Name two advantages and two disadvantages of using mature packages compared with ‘cutting edge’ packages for spatial data (for example sp vs sf). References "],
["spatial-class.html", "2 Geographic data in R Prerequisites 2.1 Vector data 2.2 Raster data 2.3 Coordinate Reference Systems 2.4 Units 2.5 Exercises", " 2 Geographic data in R Prerequisites This is the first practical chapter of book and therefore has software requirements. We assume you have a recent version of R installed on your computer and are comfortable using it at the command line, e.g. via an integrated development environment (IDE) such as RStudio (recommended). R/RStudio works on all major operating systems and can be installed and set-up in a few minutes on most modern computers, as described in section 2.3 and section 2.5 of Gillespie and Lovelace (2016) (other guides are available). If you are not a regular R user is worth taking time to ensure that you have an efficient R workflow because this will make the subsequent worked examples easier to run on your own computer, e.g. as part of an RStudio ‘project’ such as that provided in the root directory of the geocompr GitHub repository. We recommend getting up-to-speed with the language, with reference to resources such as Gillespie and Lovelace (2016), Grolemund and Wickham (2016), and online interactive guides such as that provided by DataCamp, before proceeding with the chapter. After R is installed and set-up, packages which extend R must be installed and loaded for it to handle spatial data. On Mac and Linux operating systems there are a few additional requirements: see the README of the sf package for instructions. The sf, raster, and spData packages used in this chapter can be installed and loaded with the following commands: install.packages(&quot;sf&quot;) install.packages(&quot;raster&quot;) install.packages(&quot;spData&quot;) library(sf) library(raster) library(spData) This chapter will provide brief explanations of both the fundamental geographic data types: vector and raster. A brief abstract description is provided of each before moving quickly to their implementation in R packages designed specifically for handling them. Both are vital to the sciences, although which will be of most use will largely depend on your discipline: Because human settlements and boundaries tend to be complex with precise borders defined by legal systems, vector data tends to dominate in the social sciences. In environmental sciences, by contrast, raster data tend to dominate due to its links to remote sensing. However, there is a substantial level of overlap: ecologists and demographers, for example, commonly use both vector and raster geographical data types. We therefore strongly recommend learning about each type of data before proceeding to understand how to manipulate them in subsequent chapters. 2.1 Vector data Vector data are based on points that are located on a Cartesian (or geographic) coordinate system. Each point in vector data is typically described by two numbers representing distance from the \\(origin\\) along the \\(x\\) (horizontal) and \\(y\\) (vertical) axis in Euclidean space.9 In mathematical notation these points are typically represented as numbers separated by commas and enclosed by a pair of brackets: \\((1, 3)\\) for example, represents a point located one unit to the right and three units above the origin. There is clear link between these vector points and the vector class in R. The following line of code, for example, creates a 2 dimensional vector: p = vector(mode = &quot;numeric&quot;, length = 2) More commonly one would read-in data with functions such as read_csv() from the tidyverse or read_sf() from the sf package, covered in chapter 5. To generate new data (e.g. for testing), one would more commonly use the command c() (think of ‘c’ for ‘combine’), as illustrated below:10 p = c(1, 3) Now this can be plotted in Cartesian space, as illustrated in figure 2.1: plot(p[1], p[2], xlim = c(0, 5), ylim = c(0, 5)) Figure 2.1: Illustration of vector point data in base R. Generally vector datasets have a high level of precision (but not necessarily accuracy as we will see in 2.4). Raster datasets, by contrast, use cells that break the surface up into a cells of constant size (resolution). Rasters therefore aggregate spatially specific features to a given resolution resulting in small features being blurred or lost in raster datasets. This book uses sf and raster packages to work with vector data and raster datasets respectively. 2.1.1 An introduction to simple features Simple features is an open standard developed and endorsed by the Open Geospatial Consortium (OGC) to represent a wide range of geographical information. It is a hierarchical data model that simplifies geographic data by condensing a complex range of geographic forms into a single geometry class. Only 7 out of 68 possible types of simple feature are currently used in the vast majority of GIS operations (Figure 2.2). All of these are fully supported (with plotting methods etc) in the R package sf (Pebesma 2017).11 Figure 2.2: The subset of the Simple Features class hierarchy supported by sf. Figure based on the Open Geospatial Consortium document 06-103r4. sf can represent all common vector geometry types (raster data classes are not supported by sf): points, lines, polygons and their respective ‘multi’ versions (which group together features of the same type into a single feature). sf also supports geometry collections, which can contain multiple geometry types in a single feature. Given the breadth of geographic data forms, it may come as a surprise that a class system to support all of them is provided in a single package, which can be installed from CRAN:12 sf incorporates the functionality of the 3 main packages of the sp paradigm (sp (Pebesma and Bivand 2017) for the class system, rgdal (Bivand, Keitt, and Rowlingson 2017) for reading and writing data, rgeos (Bivand and Rundel 2017) for spatial operations undertaken by GEOS) in a single, cohesive whole. This is well-documented in sf’s vignettes: vignette(&quot;sf1&quot;) # for an introduction to the package vignette(&quot;sf2&quot;) # for reading, writing and converting Simple Features vignette(&quot;sf3&quot;) # for manipulating Simple Features As the first vignette explains, simple feature objects in R are stored in a data frame, with geographic data occupying special column, a ‘list-column’. This column is usually named ‘geom’ or ‘geometry’. A ‘real world’ example is loaded by the spData package, which loads the world object: library(spData) data(&quot;world&quot;) In the above code spData silently loaded the world dataset (and many other spatial datasets - see the spData website for a full list). The dataset contains spatial and non-spatial information, as shown by the function names(), which reports the column headings in data frames. This can be seen as the final column name of world: names(world) #&gt; [1] &quot;iso_a2&quot; &quot;name_long&quot; &quot;continent&quot; &quot;region_un&quot; &quot;subregion&quot; #&gt; [6] &quot;type&quot; &quot;area_km2&quot; &quot;pop&quot; &quot;lifeExp&quot; &quot;gdpPercap&quot; #&gt; [11] &quot;geom&quot; It is the contents of this modest-looking geom column that gives sf objects their spatial powers. It’s actually a list-column, containing all the coordinates needed to plot the result as a map using the plot() method, the results of which are presented in Figure 2.3. library(sf) # must be loaded to plot sf objects #&gt; Linking to GEOS 3.5.0, GDAL 2.1.0, proj.4 4.8.0 plot(world) #&gt; Warning: plotting the first 9 out of 10 attributes; use max.plot = 10 to #&gt; plot all Figure 2.3: A spatial plot of the world using the sf package, with a facet for each attribute. Note that instead of creating a single map, as most GIS programs would, the plot() command has created multiple maps, one for each variable in the world datasets. This behavior can be useful for exploring the spatial distribution of different variables and is discussed further in 2.1.3 below. Being able to treat spatial objects as regular data frames with spatial powers has many advantages, especially if you are already used to working with data frames. The commonly used summary() function, for example, provides a useful overview of the variables within the world object, but prefixed with some information about the object’s spatial component: summary(world[&quot;lifeExp&quot;]) #&gt; lifeExp geom #&gt; Min. :48.9 MULTIPOLYGON :177 #&gt; 1st Qu.:64.3 epsg:4326 : 0 #&gt; Median :72.8 +proj=long...: 0 #&gt; Mean :70.6 #&gt; 3rd Qu.:77.1 #&gt; Max. :83.6 #&gt; NA&#39;s :9 The result provides a quick summary of both the non-spatial and spatial data contained in world: the average life expectancy is 73 years (ranging from less than 50 to more than 80 years) across all countries, and these are represented by MULTIPOLYGONs, allowing many polygons per country (this is needed for countries with many islands such as Indonesia and Greece). Note that by appending [&quot;lifeExp&quot;] onto the object name in the previous code chunk, only the relevant column was summarized. We explore such ‘attribute operations’ in Chapter 3. First, it’s worth taking a look at the basic behavior and contents of this simple feature object, which can usefully be thought of as a ’Spatial dataFrame). sf objects are easy to subset. The code below shows its first 2 rows and 3 columns. The output shows 2 major differences compared with a regular data.frame: the inclusion of additional geographic data (geometry type, dimension, bbox and CRS information - epsg (SRID), proj4string), and the presence of final geometry column: world[1:2, 1:3] #&gt; Simple feature collection with 2 features and 3 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 11.6401 ymin: -17.93064 xmax: 75.15803 ymax: 38.48628 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long continent geom #&gt; 1 AF Afghanistan Asia MULTIPOLYGON(((61.210817091... #&gt; 2 AO Angola Africa MULTIPOLYGON(((16.326528354... All this may seem rather complex, especially for a class system that is supposed to be simple. However, there are good reasons for organizing things this way and using sf. 2.1.2 Why Simple Features? There are many advantages of sf over sp, including: Faster reading and writing of data (more than 10 times faster in some cases) Better plotting performance sf objects can be treated as dataframes in most operations sf functions can be combined using %&gt;% operator and works well with the tidyverse collection of R packages sf function names are relatively consistent and intuitive (all begin with st_) compared with the function names and syntax of the sp, rgdal and rgeos packages that it supersedes. A broader advantage is that simple features are so well-supported by other software products, not least PostGIS, which has heavily influenced the design of sf. A disadvantage you should be aware of, however, is that sf is not feature complete and that it continues to evolve. The transition from sp to sf will likely take many years, and many spatial packages may never switch. Even if you discover spatial data with R through the sf package, it is still worth at least being aware of sp classes, even if you rarely use them for everyday geospatial tasks. Fortunately it is easy to translate between sp and sf using the as() function: library(sp) world_sp = as(object = world, Class = &quot;Spatial&quot;) 2.1.3 Basic map making Basic maps in sf can be created quickly with the base plot() function. Unlike sp, however, sf by default creates a faceted plot, one sub-plot for each variable, as illustrated in the left-hand image in Figure 2.4. plot(world[3:4]) plot(world[&quot;pop&quot;]) Figure 2.4: Plotting with sf, with multiple variables (left) and a single variable (right). As with sp, you can add layers to your maps created with plot(), with the argument add = TRUE.13 To illustrate this, and prepare for content covered in chapters 3 and 4 on attribute and spatial data operations, we will subset and combine countries in the world object, to create a single object that represents Asia: asia = world[world$continent == &quot;Asia&quot;,] asia = st_union(asia) We can now plot the Asian continent over a map of the world. Note, however, that this only works if the initial plot has only 1 layer: plot(world[&quot;pop&quot;]) plot(asia, add = TRUE, col = &quot;red&quot;) This can be very useful for quickly checking the geographic correspondence between two or more layers: the plot() function is fast to execute and requires few lines of code, but does not create interactive maps with a wide range of options. For more advanced map making we recommend using a dedicated visualization package such as tmap, ggplot2, mapview, or leaflet. 2.1.3.1 Further work sf makes R data objects more closely aligned to the data model used in GDAL and GEOS, in theory making spatial data operations faster. The work here provides a taste of the way sf operates but there is much more to learn (see Chapter 4). And there is also a wealth of information in the highly recommended vignettes of the package. As a final exercise, we’ll see one way of how to do a spatial overlay in sf. First, we convert the countries of the world into centroids, and then subset those in Asia. Finally, the summary-command tells us how many centroids (countries) are part of Asia (43) and how many are not (134). world_centroids = st_centroid(world) #&gt; Warning in st_centroid.sfc(st_geometry(x)): st_centroid does not give #&gt; correct centroids for longitude/latitude data sel_asia = st_intersects(world_centroids, asia, sparse = FALSE) #&gt; although coordinates are longitude/latitude, it is assumed that they are planar summary(sel_asia) #&gt; V1 #&gt; Mode :logical #&gt; FALSE:134 #&gt; TRUE :43 Note: another way of achieving the same result is with a GEOS function for identifying spatial overlay, which we’ll cover in more detail in Chapter 4. sf’s plot() function builds on base plotting methods, allowing access to its many optional arguments (see ?plot). This provides powerful but not necessarily intuitive functionality, as illustrated by the variable circle sizes generated by the cex argument (see Figure 2.5, generated by the code below). plot(world[&quot;continent&quot;]) plot(world_centroids, add = TRUE, cex = world$pop / 1e8, lwd = 3) Figure 2.5: Centroids representing country population, diameter being proportional to population. 2.1.4 Simple feature classes To understand new data formats in depth, it often helps to generate them for first principles. This section walks through vector spatial classes step-by-step, from the elementary simple feature geometry to simple feature objects, with class sf, representing complex spatial data. Before describing each geometry type that the sf package supports it is worth taking a step back to understand the building blocks of sf objects. As stated in section 2.1.1, simple features are simply dataframes with at least one special column that makes it spatial. These spatial columns are often called geom or geometry and can be like non-spatial columns: world$geom refers to the spatial element of the world object described above. These geometry columns are ‘list columns’ of class sfc: they are simple feature collections. In turn, sfc objects are composed of one or more objects of class sfg: simple feature geometries. To understand how the spatial components of simple features work, it is vital to understand simple feature geometries. For this reason we cover each type currently supported sfg in the next sections before moving to describe how they can be combined to form sfc and eventually full sf objects. 2.1.4.1 Simple feature geometry types Geometries are the basic building blocks of simple features. Simple features could be represented as one of the 17 geometry types using the sf package. In this chapter we will focus on seven, the most commonly used, simple features types: POINT, LINESTRING, POLYGON, MULTIPOINT, MULTILINESTRING, MULTIPOLYGON and GEOMETRYCOLLECTION. The whole list of possible feature types could be found in the PostGIS manual. Simple features, outside of an R environment, could be represented in one of two ways, ether as a well-known binary (WKB) or well-known text (WKT). Well-known binary (WKB) representations are usually hexadecimal strings, which are used to transfer and store geometry objects in databases. Well-known text (WKT), on the other hand, is a text markup description of simple features. Both formats are exchangeable, therefore we would focus only on the well-known text (WKT) representation. All of the geometry types are built of points. Each point could be described as coordinates in a 2D, 3D or 4D space: X and Y coordinates represent location of points. It could be either easting and northing or longitude and latitude. A Z coordinate denotes altitude. An M coordinate (“measure”) allows to represent some measure associated to the point, but not the whole feature. This coordinate could be a time of measurement, information who measured this point or what’s the error of measurement. As a result, four possible types of points exist - XY (two-dimensional), XYZ (three-dimensional containing altitude), XYM (three-dimensional containing measure) and XYZM (four-dimensional). They could be easily describes as a well-known text: XY - POINT (5 2) XYZ - POINT (5 2 3) XYM - POINTM (5 2 1) XYZM - POINT (5 2 3 1) A linestring is represented by a sequence of points with linear interpolation between points, for example: LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2) Linestring cannot have self intersecting line part. In other words, lines shouldn’t pass through the same point twice (except for the endpoint). A polygon is a sequence of points, where the first and last point have the same coordinates. Similarly to linestring, polygon needs to create a non-self intersecting ring. By the definition, polygon has one exterior boundary (outer ring) and zero or more interior boundaries (inner ring). These interior boundaries are often known as holes. Zero interior boundaries (holes) - POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5)) One hole - POLYGON ((1 5, 4 4, 4 1, 2 2, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) The next three geometry types are the sets of previous ones: a multipoint is a set of points, multilinestring is a set strings and multipolygon is a set of polygons: Multipoint - MULTIPOINT (5 2, 1 3, 3 4, 3 2) Multistring - MULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4)) Multipolygon - MULTIPOLYGON ((1 5, 4 4, 4 1, 2 2, 1 5), (0 2, 1 2, 1 3, 0 3, 0 2)) A geometry collection is the most heterogeneous type. It could consists of a set of any geometry types previously mentioned, for example: Geometry collection - GEOMETRYCOLLECTION (MULTIPOINT (5 2, 1 3, 3 4, 3 2), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2))) 2.1.4.2 Simple feature geometry (sfg) objects Simple feature geometry types are represented in R by objects of a sfg class. A sfg object is a geometry of a single feature - a point, linestring, polygon, multipoint, multilinestring, multipolygon or geometry collection. Usually you don’t need to create geometries on your own. In most cases, geometries are read from spatial files. However, a set of function to create simple feature geometry objects (sfg) exists in R and could be used in special cases. The names of these functions are simple and consistent, as they start with a st_ prefix and end with name of geometry types in lowercase letters: A point - st_point() A linestring - st_linestring() A polygon - st_polygon() A multipoint - st_multipoint() A multilinestring - st_multilinestring() A multipolygon - st_multipolygon() A geometry collection - st_geometrycollection() In R, sfg objects are represented by three native data types: A numeric vector - a single point A matrix - a set of points, where each row contains a point - a multipoint or linestring A list - any other set, e.g. a multilinestring or geometry collection To create point objects, we could use the st_point() function: # note that we use a numeric vector for points st_point(c(5, 2)) # XY point #&gt; POINT(5 2) st_point(c(5, 2, 3)) # XYZ point #&gt; POINTZ(5 2 3) st_point(c(5, 2, 1), dim = &quot;XYM&quot;) # XYM point #&gt; POINTM(5 2 1) st_point(c(5, 2, 3, 1)) # XYZM point #&gt; POINTZM(5 2 3 1) XY, XYZ and XYZM types of points are automatically created based on the length of a numeric vector. Only the XYM type needs to be specified using a dim argument. Multipoint and linestring objects are created based on a matrix using st_multipoint() and st_linestring() functions: # the rbind function simplify creation of matrices ## MULTIPOINT multipoint_matrix = rbind(c(5, 2), c(1, 3), c(3, 4), c(3, 2)) st_multipoint(multipoint_matrix) #&gt; MULTIPOINT(5 2, 1 3, 3 4, 3 2) ## LINESTRING linestring_matrix = rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)) st_linestring(linestring_matrix) #&gt; LINESTRING(1 5, 4 4, 4 1, 2 2, 3 2) The rest of objects are represented by lists: ## POLYGON polygon_list = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))) st_polygon(polygon_list) #&gt; POLYGON((1 5, 2 2, 4 1, 4 4, 1 5)) ## POLYGON with a hole polygon_border = rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)) polygon_hole = rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4)) polygon_with_hole_list = list(polygon_border, polygon_hole) st_polygon(polygon_with_hole_list) #&gt; POLYGON((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) ## MULTILINESTRING multilinestring_list = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), rbind(c(1, 2), c(2, 4))) st_multilinestring((multilinestring_list)) #&gt; MULTILINESTRING((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4)) ## MULTIPOLYGON multipolygon_list = list(list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))), list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2)))) st_multipolygon(multipolygon_list) #&gt; MULTIPOLYGON(((1 5, 2 2, 4 1, 4 4, 1 5)), ((0 2, 1 2, 1 3, 0 3, 0 2))) ## GEMETRYCOLLECTION gemetrycollection_list = list(st_multipoint(multipoint_matrix), st_linestring(linestring_matrix)) st_geometrycollection(gemetrycollection_list) #&gt; GEOMETRYCOLLECTION(MULTIPOINT(5 2, 1 3, 3 4, 3 2), LINESTRING(1 5, 4 4, 4 1, 2 2, 3 2)) 2.1.4.3 Simple feature collections One sfg object contains only a single simple feature geometry. A simple feature collection (sfc) is a list of sfg objects with information about a coordinate reference system. The st_sfc() function can be used to create sfc objects, as illustrated in the code below, which combines two simple features of a point type into a single feature: # sfc POINT point1 = st_point(c(5, 2)) point2 = st_point(c(1, 3)) st_sfc(point1, point2) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT(5 2) #&gt; POINT(1 3) In most cases, an sfc object contains objects of identical geometry type. Therefore, when we combine sfg objects of a polygon type we would get sfc of a polygon type, and a collection of multilinestring would result into sfc of a multilinestring type: # sfc POLYGON polygon_list1 = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))) polygon1 = st_polygon(polygon_list) polygon_list2 = list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2))) polygon2 = st_polygon(polygon_list2) st_sfc(polygon1, polygon2) #&gt; Geometry set for 2 features #&gt; geometry type: POLYGON #&gt; dimension: XY #&gt; bbox: xmin: 0 ymin: 1 xmax: 4 ymax: 5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POLYGON((1 5, 2 2, 4 1, 4 4, 1 5)) #&gt; POLYGON((0 2, 1 2, 1 3, 0 3, 0 2)) # sfc MULTILINESTRING multilinestring_list1 = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), rbind(c(1, 2), c(2, 4))) multilinestring1 = st_multilinestring((multilinestring_list1)) multilinestring_list2 = list(rbind(c(2, 9), c(7, 9), c(5, 6), c(4, 7), c(2, 7)), rbind(c(1, 7), c(3, 8))) multilinestring2 = st_multilinestring((multilinestring_list2)) st_sfc(multilinestring1, multilinestring2) #&gt; Geometry set for 2 features #&gt; geometry type: MULTILINESTRING #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 1 xmax: 7 ymax: 9 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; MULTILINESTRING((1 5, 4 4, 4 1, 2 2, 3 2), (1 2... #&gt; MULTILINESTRING((2 9, 7 9, 5 6, 4 7, 2 7), (1 7... It is possible to create a sfc object from sfg objects with different geometry types. This new object would have a “geometry” geometry type: # sfc GEOMETRY st_sfc(point1, multilinestring1) #&gt; Geometry set for 2 features #&gt; geometry type: GEOMETRY #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 1 xmax: 5 ymax: 5 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT(5 2) #&gt; MULTILINESTRING((1 5, 4 4, 4 1, 2 2, 3 2), (1 2... The simple feature collection objects could have more information about spatial data than just geometries. It is possible to store coordinate reference systems (CRS) in them. CRS can be represented by epsg (SRID) and proj4string attributes. The default value of epsg (SRID) and proj4string is NA (Not Available), which is used when the CRS is unknown: st_sfc(point1, point2) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): NA #&gt; proj4string: NA #&gt; POINT(5 2) #&gt; POINT(1 3) The sfc object could have NA values in both attributes or have an actual value for one or two CRS attributes. Importantly, all geometries in the sfc objects must have the same CRS. We can add coordinate reference system as a crs argument of st_sfc(). This argument could accept either an integer with the epsg code or character with proj4string. For example, we can set the WGS 84 using either its epsg code (4326) or its proj4string definition (&quot;+proj=longlat +datum=WGS84 +no_defs&quot;): # EPSG definition st_sfc(point1, point2, crs = 4326) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; POINT(5 2) #&gt; POINT(1 3) # PROJ4STRING definition st_sfc(point1, point2, crs = &quot;+proj=longlat +datum=WGS84 +no_defs&quot;) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; POINT(5 2) #&gt; POINT(1 3) Both of these approaches have advantages and disadvantages. An epsg code is usually shorter and therefore easier to remember. The code also refers to only one, well-defined coordinate reference system. It could be, however, considered a limitation of epsg, as it is not flexible. On the hand, a proj4string definition is longer - it could specify many different parameters, such as projection type, datum and ellipsoid. This makes proj4string more complicated, but also allows to create many different projections and modify existing ones. proj4string is also supported by the PROJ.4 software (and therefore by the sf package), which enables transformations between different projections. epsg always points to a particular CRS. This property makes possible for PROJ.4 to convert epsg into corresponding proj4string. For example, we can set the UTM Zone 11N projection with epsg code 2955: st_sfc(point1, point2, crs = 2955) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): 2955 #&gt; proj4string: +proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; POINT(5 2) #&gt; POINT(1 3) As you can see above, the proj4string definition was automatically added. Now we can try to set the CRS using proj4string: st_sfc(point1, point2, crs = &quot;+proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot;) #&gt; Geometry set for 2 features #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 1 ymin: 2 xmax: 5 ymax: 3 #&gt; epsg (SRID): NA #&gt; proj4string: +proj=utm +zone=11 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; POINT(5 2) #&gt; POINT(1 3) The above result doesn’t contain epsg. This is due the fact that no general method for conversion from proj4string to epsg exists. 2.1.4.4 Simple feature objects Most of the time, geometries are related to a set of attributes. These attributes could represent the name of the geometry, measured value, group to which the geometry belongs, and many more. For example, we measured a temperature of 25°C on the Trafalgar Square in London on June 21th 2017. This can be described not only by its coordinates and temperature value, but also by the name of the point, the date of the measurement, it’s category (city or village), or if the measurement was made using an automatic station. The simple feature class, sf, is a combination of an attribute table (data.frame) and simple feature geometry collection (sfc). Simple features are created using the st_sf() function: # sfg objects london_point = st_point(c(0.1, 51.5)) ruan_point = st_point(c(-9, 53)) # sfc object our_geometry = st_sfc(london_point, ruan_point, crs = 4326) # data.frame object our_attributes = data.frame(name = c(&quot;London&quot;, &quot;Ruan&quot;), temperature = c(25, 13), date = c(as.Date(&quot;2017-06-21&quot;), as.Date(&quot;2017-06-22&quot;)), category = c(&quot;city&quot;, &quot;village&quot;), automatic = c(FALSE, TRUE)) # sf object sf_points = st_sf(our_attributes, geometry = our_geometry) The above example illustrates the components of sf objects. Firstly, simple feature geometry (sfg) objects are defined using coordinates. These objects are combined into a simple feature collection (sfc). The sfc also stores the information about coordinate reference system. data.frame is created, where each row corresponds to one geometry feature. Finally, the attribute table and sfc object are tied together using the st_sf() function. sf_points #&gt; Simple feature collection with 2 features and 5 fields #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: -9 ymin: 51.5 xmax: 0.1 ymax: 53 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; name temperature date category automatic geometry #&gt; 1 London 25 2017-06-21 city FALSE POINT(0.1 51.5) #&gt; 2 Ruan 13 2017-06-22 village TRUE POINT(-9 53) The resulting object has two classes - sf and data.frame: class(sf_points) #&gt; [1] &quot;sf&quot; &quot;data.frame&quot; The result shows that sf objects actually have two classes, sf and data.frame. Simple features are simply data frames (square tables), but with spatial attributes (usually stored in a special geom list-column in the data frame). This duality is central to the concept of simple features: most of the time a sf can be treated as and behaves like a data.frame. Simple features are, in essence, data frames with a spatial extension. 2.2 Raster data 2.2.1 An introduction to raster Raster objects in R are supported by the raster package. It provides functions to create, read, processed and write of raster datasets. Beside the general raster data manipulation, raster provides many low level functions that can be used to create and develop new concepts. raster also supports work on large raster datasets that are stored on a hard drive, but are too large to fit into memory. Instead of recreating the whole file in RAM, this package extracts information about the structure of the dataset, such as a number of rows and columns, spatial extent and the name of the file. When manipulating this dataset, values are read and processed in a small chunk and written either to a specified file on a disk or temporary file. The list of the raster function could be found using /home/travis/R/Library/raster/help/raster-package. 2.2.2 Basic map making 2.2.3 Raster classes The raster package provides three main classes of objects - RasterLayer, RasterBrick and RasterStack. RasterLayer represents the simplest raster object, consisting of only one layer and store information about a number of rows and columns, spatial object extent and coordinate reference system used. This raster class could store raster values in a RAM memory or only point to a file on hard drive that holds the values. Object of the RasterLayer class is created by the raster() function: library(raster) #&gt; Loading required package: sp r = raster() # creation of an empty RasterLayer object r #&gt; class : RasterLayer #&gt; dimensions : 180, 360, 64800 (nrow, ncol, ncell) #&gt; resolution : 1, 1 (x, y) #&gt; extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 # creation of the RasterLayer object with a given number of columns and rows, and extent r1 = raster(ncol = 101, nrow = 77, xmn = 0, xmx = 101, ymn = 0, ymx = 77) r1 #&gt; class : RasterLayer #&gt; dimensions : 77, 101, 7777 (nrow, ncol, ncell) #&gt; resolution : 1, 1 (x, y) #&gt; extent : 0, 101, 0, 77 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 values(r1) = sample(1:ncell(r1)) # adding random values to the new raster object Two additional classes, RasterBrick and RasterStack are used when dealing with multiple layers. These two classes differ in terms of a number of supported files, type of representation and processing speed. A RasterBrick contain multiple layers of raster data, which refer to only a single, mutlilayer file. RasterBrick objects are created using the brick() function. This function usually takes a filename to a multilayer raster file. However, it is also possible to provide a Raster* object, array, and a few more. All of possible formats could be found on the help file - ?brick. multilayer_raster_filepath = system.file(&quot;external/rlogo.grd&quot;, package=&quot;raster&quot;) r_brick = brick(multilayer_raster_filepath) r_brick #&gt; class : RasterBrick #&gt; dimensions : 77, 101, 7777, 3 (nrow, ncol, ncell, nlayers) #&gt; resolution : 1, 1 (x, y) #&gt; extent : 0, 101, 0, 77 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=merc +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 #&gt; data source : /home/travis/R/Library/raster/external/rlogo.grd #&gt; names : red, green, blue #&gt; min values : 0, 0, 0 #&gt; max values : 255, 255, 255 The nlayers function helps to get the number of layers in a Raster* object: nlayers(r_brick) #&gt; [1] 3 A RasterStack is a list of RasterLayer objects with the same extent and resolution. It can be created based on a group of object from many sources - different files, another bands in a multi-band file and RasterLayer objects created in R. raster_on_disk = raster(multilayer_raster_filepath, layer = 1) raster_in_memory = raster(xmn = 0, xmx = 101, ymn = 0, ymx = 77, res = 1) values(raster_in_memory) = sample(1:ncell(raster_in_memory)) crs(raster_in_memory) = crs(raster_on_disk) r_stack &lt;- stack(raster_in_memory, raster_on_disk) r_stack #&gt; class : RasterStack #&gt; dimensions : 77, 101, 7777, 2 (nrow, ncol, ncell, nlayers) #&gt; resolution : 1, 1 (x, y) #&gt; extent : 0, 101, 0, 77 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=merc +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 #&gt; names : layer, red #&gt; min values : 1, 0 #&gt; max values : 7777, 255 Due to its properties, RasterBrick objects should be processed in a shorter time than RasterStack. Additionally, operations on RasterBrick and RasterStack objects will typically return RasterBrick. On the other hand, RasterStack give more flexibility, as a single object could be related to data stored in a memory and on disk in the same time. RasterBrick objects could be stored only in a memory or on disk. 2.3 Coordinate Reference Systems This section is work in progress. Despite the differences between vector and raster spatial data types, they are united by shared concepts intrinsic to spatial data. Perhaps the most important of these is Coordinate Reference System (CRS), which defines how the spatial elements of the data relate to the surface of the Earth (or other body). In sf the CRS of an object can be retrieved and set using st_crs() and st_set_crs() respectively: old_crs = st_crs(sf_points) # get CRS old_crs # print CRS #&gt; $epsg #&gt; [1] 4326 #&gt; #&gt; $proj4string #&gt; [1] &quot;+proj=longlat +datum=WGS84 +no_defs&quot; #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;crs&quot; sf_points = st_set_crs(sf_points, 27700) # set CRS #&gt; Warning: st_crs&lt;- : replacing crs does not reproject data; use st_transform #&gt; for that Note the warning emitted after the CRS for sf_points was set to 27700. This is a good thing: we have imposed a spatial reference onto data without knowing what that means. To discover what the ‘magic number’ 27700 means, we can retrieve the CRS again (see Chapter ?? for more on CRSs): st_crs(sf_points) #&gt; $epsg #&gt; [1] 27700 #&gt; #&gt; $proj4string #&gt; [1] &quot;+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +towgs84=446.448,-125.157,542.06,0.15,0.247,0.842,-20.489 +units=m +no_defs&quot; #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;crs&quot; 2.4 Units The final thing to say about sf objects in this chapter is that they have units. This is advantageous because it prevents confusion caused by the fact that different CRSs use different units (most use meters, some use feet). Furthermore, it also provides information on dimensionality, as illustrated by calculating the area of Nigeria: nigeria = world[world$name_long == &quot;Nigeria&quot;,] st_area(nigeria) #&gt; 9.05e+11 m^2 The result, as expected, is in units of square meters (m2), representing 2 dimensional space, and that Nigeria is a large country! To translate the huge number into a more digestible size, it is tempting to divide the results by a million (the number of square meters in a square kilometer): st_area(nigeria) / 1e6 #&gt; 905072 m^2 However, the result is incorrectly given in the same units. The solution is to set the units with the units package: units::set_units(st_area(nigeria), km^2) #&gt; 905072 km^2 2.5 Exercises What does the summary of the geometry column tell us about the world dataset, in terms of: The geometry type? How many countries there are? The coordinate reference system (CRS)? Using sf’s plot() command, create a map of Nigeria in context, building on the code that creates and plots Asia above (see Figure ?? for an example of what this could look like). Hint: this used the lwd, main and col arguments of plot(). Bonus: make the country boundaries a dotted grey line. Hint: border is an additional argument of plot() for sf objects. Exercise 3 What does the lwd argument do in the plot() code that generates Figure 2.5. Perform the same operations and map making for another continent of your choice. Bonus: Download some global geographic data and add attribute variables assigning them to the continents of the world. References "],
["attr.html", "3 Attribute data operations Prerequisites 3.1 Introduction 3.2 Attribute subsetting 3.3 Attribute data aggregation 3.4 Attribute data joining 3.5 Attribute data creation 3.6 Removing spatial information 3.7 Exercises", " 3 Attribute data operations Prerequisites This chapter requires tidyverse and sf: library(sf) library(tidyverse) You must have loaded the world and worldbank_df data which are loaded automatically by the spData package: library(spData) 3.1 Introduction Attribute data is non-spatial information associated with geographic data. In the context of simple features, introduced in the previous chapter, this means a data frame with a column for each variable and one row per geographic feature stored in the geom list-column of sf objects. This structure enables multiple columns to represent a range of attributes for thousands of features (one row per feature). There is a strong overlap between geographical and non-geographical operations: non-spatial subset, aggregate and join each have their geographical equivalents. The subsetting functions [ from base R and filter() from the tidyverse, for example, can also be used for spatial subsetting: the skills are cross-transferable. This chapter therefore provides the foundation for Chapter 4, in terms of structure and input data. As outlined in Chapter 2, support for simple features in R is provided by the sf package. sf ensures simple feature objects work well with generic R functions such as plot() and summary(). The reason for this is that simple features have their own class, which behave simultaneously as geographic data objects (e.g. plotting as maps) and square tables (e.g. with attribute columns referred to with the $ operator). The trusty data.frame (and extensions to it such as the tibble class used in the tidyverse) is a workhorse for data analysis in R. Extending this system to work with spatial data has many advantages, meaning that all the accumulated know-how in the R community for handling data frames to be applied to geographic data which contain attributes. Before proceeding to perform various attribute operations of a dataset, it is worth taking time to think about its basic parameters. In this case, the world object contains 10 non-geographical columns (and one geometry list-column) with data for almost 200 countries. This can be be checked using base R functions for working with tabular data such as nrow() and ncol(): dim(world) # it is a 2 dimensional object, with rows and columns #&gt; [1] 177 11 nrow(world) # how many rows? #&gt; [1] 177 ncol(world) # how many columns? #&gt; [1] 11 Extracting the attribute data of an sf object is the same as removing its geometry: world_df = st_set_geometry(world, NULL) class(world_df) #&gt; [1] &quot;data.frame&quot; This can be useful if the geometry column causes problems, e.g. by occupying large amounts of RAM, or to focus attention on the non-spatial data. For most cases, however, there is no harm in keeping the geometry column because non-spatial data operations on sf objects act only on the attribute data. For this reason, being good at working with attribute data in geographic data is the same being proficient at handling data frames in R. For many applications, the most effective and intuitive way of working with data frames is with the dplyr package, as we will see in the next section.14 3.2 Attribute subsetting Because simple feature objects are also data frames, you can use a wide range of functions (from base R and packages) for subsetting them, based on attribute data. Base R subsetting functions include [, subset() and $. dplyr subsetting functions include select(), filter(), and pull(). Both sets of functions preserve the spatial components of the data. The [ operator subsets rows and columns. It requires two arguments, one for rows (observations) and one for columns (variables), and is appended to the object name, e.g. object[rows, columns], which can be either numeric, indicating position, or character, indicating row or column names. Leaving an argument empty returns all, meaning object[rows,] returns just the rows of interest for all columns. This functionality is demonstrated below (results not shown - try running this on your own computer to check the output is as expected): world[1:6,] # subset rows by position world[, 1:3] # subset columns by position world[, c(&quot;name_long&quot;, &quot;lifeExp&quot;)] # subset columns by name The [ subsetting operator also accepts logical vectors corresponding to some criteria which returns TRUE or FALSE. The following code chunk, for example, creates a new object, small_countries, which only contains nations whose surface area is below 100,000 km2: sel_area = world$area_km2 &lt; 10000 summary(sel_area) #&gt; Mode FALSE TRUE #&gt; logical 170 7 small_countries = world[sel_area,] Note that we created the intermediary sel_object to illustrate the process and demonstrate that only 7 countries are ‘small’ by this definition. A more concise command, that omits the intermediary object, generates the same result: small_countries = world[world$area_km2 &lt; 10000,] Another way to generate the same result is with the base R function subset(): small_countries = subset(world, area_km2 &lt; 10000) The $ operator retrieves a variable by its name and returns a vector: world$name_long dplyr makes working with data frames easier and is compatible with sf objects. The main dplyr functions that help with attribute subsetting are select(), slice(), filter() and pull(). The select() function picks columns by name or position. For example, you could select only two columns, name_long and pop, with the following command: world1 = select(world, name_long, pop) head(world1, n = 2) #&gt; Simple feature collection with 2 features and 2 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 11.6401 ymin: -17.93064 xmax: 75.15803 ymax: 38.48628 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; name_long pop geom #&gt; 1 Afghanistan 31627506 MULTIPOLYGON(((61.210817091... #&gt; 2 Angola 24227524 MULTIPOLYGON(((16.326528354... This function allows a range of columns to be selected using the : operator: # all columns between name_long and pop (inclusive) world2 = select(world, name_long:pop) head(world2, n = 2) Specific columns can be omitted using the - operator: # all columns except subregion and area_km2 (inclusive) world3 = select(world, -subregion, -area_km2) head(world3, n = 2) select() can be also used to both subset and rename columns in a single line, for example: world4 = select(world, name_long, population = pop) head(world4, n = 2) #&gt; Simple feature collection with 2 features and 2 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 11.6401 ymin: -17.93064 xmax: 75.15803 ymax: 38.48628 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; name_long population geom #&gt; 1 Afghanistan 31627506 MULTIPOLYGON(((61.210817091... #&gt; 2 Angola 24227524 MULTIPOLYGON(((16.326528354... This is more concise than the base R equivalent (which saves the result as an object called world5 to avoid overriding the world dataset created previously): world5 = world[c(&quot;name_long&quot;, &quot;pop&quot;)] # subset columns by name names(world5)[3] = &quot;population&quot; # rename column manually The select() function works with a number of special functions that help with more complicated selection, such as contains(), starts_with(), num_range(). More details could be find on the function help page - ?select. slice() is the equivalent of select() but work for rows. The following code chunk, for example, selects the 3rd to 5th rows: slice(world, 3:5) filter() is dplyr’s equivalent of base R’s subset() function. It keeps only rows matching given criteria, e.g. only countries with a very high average life expectancy: # only countries with a life expectation larger than 82 years world6 = filter(world, lifeExp &gt; 82) The standard set of comparison operators can be used in the filter() function: Symbol Name == Equal to != Not equal to &gt; Greater than &gt;= Greater than or equal &lt; Less than &lt;= Less than or equal &amp; And | Or ! Not , >=, The pipe operator (%&gt;%), which passes the output of one function into the first argument of the next function, is commonly used in dplyr data analysis workflows. This works because the fundamental dplyr functions (or ‘verbs’, like select()) all take a data frame object in and spit a data frame object out. Combining many functions together with pipes is called chaining or piping. The advantage over base R for complex data processing operations is that this approach prevents nested functions and is easy to read because there is a clear order and modularity to the work (a piped command can be commented out, for example). The example below shows yet another way of creating the renamed world dataset, using the pipe operator: world7 = world %&gt;% select(name_long, continent) Note that this can also be written without the pipe operator because, in the above code, the world object is simply ‘piped’ into the first argument of select(). The equivalent dplyr code without the pipe operator is: world8 = select(world, name_long, continent) pull() retrieves a single variable by name or position and returns a vector: world %&gt;% pull(name_long) The pipe operator can be used for many data processing tasks with attribute data. # 1,000,000,000 could be expressed as 1e9 in the scientific notation world %&gt;% filter(pop &gt; 1e9) #&gt; Simple feature collection with 2 features and 10 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 68.17665 ymin: 7.965535 xmax: 135.0263 ymax: 53.4588 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long continent region_un subregion type #&gt; 1 CN China Asia Asia Eastern Asia Country #&gt; 2 IN India Asia Asia Southern Asia Sovereign country #&gt; area_km2 pop lifeExp gdpPercap geom #&gt; 1 9409832 1.36e+09 75.8 12759 MULTIPOLYGON(((110.33918786... #&gt; 2 3142892 1.30e+09 68.0 5392 MULTIPOLYGON(((77.837450799... This is equivalent to the following base R code (not run to preserve the NAs):15 # subsetting simple feature rows by values world$pop[is.na(world$pop)] = 0 # set NAs to 0 world_few_rows = world[world$pop &gt; 1e9,] The %&gt;% operator works the best for combining many operations. For example, we want to (1) rename the name_long column into a name column, (2) picks only name, subregion and gdpPercap and (3) subset countries from “Eastern Asia” with gross domestic product per capita larger than 30,000$: world %&gt;% select(name = name_long, subregion, gdpPercap) %&gt;% filter(subregion == &quot;Eastern Asia&quot;, gdpPercap &gt; 30000) #&gt; Simple feature collection with 2 features and 3 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: 126.1174 ymin: 31.02958 xmax: 145.5431 ymax: 45.55148 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; name subregion gdpPercap geom #&gt; 1 Japan Eastern Asia 37365 MULTIPOLYGON(((134.63842817... #&gt; 2 Republic of Korea Eastern Asia 33640 MULTIPOLYGON(((128.34971642... 3.3 Attribute data aggregation As demonstrated in chapter 2, summary() provides a quick summary of the spatial and non-spatial components of spatial objects. Enter the following command to for an overview of the world object and all its variables (result not shown): summary(world) This function is useful when using R interactively, but lacks flexibility and should not be used to create new objects. The dplyr equivalent is summarize(), which returns summary statistics of groups and variables defined by the user. The following code, for example, calculates the total population and number of all countries in the world: # customized data summary world_summary = world %&gt;% summarize(pop = sum(pop, na.rm = TRUE), country_n = n()) world_summary #&gt; Simple feature collection with 1 feature and 2 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; pop country_n geom #&gt; 1 7.21e+09 177 MULTIPOLYGON(((-159.2081835... The new object, world_summary contains only one feature, representing all 177 countries. The new object contains 2 variables representing the total population and number of countries of the world, created by the pop = and country_n = arguments in the summarize() function call above. summarize() allows a wide range of summary statistics to be generated. A list of useful summary statistics can be found in the help page associated with the function: see ?summarize for more information. summarize() becomes even more powerful when combined with group_by(), allowing per group summaries, analogous to the base R function aggregate(). The following code chunk calculates the total population and number of countries on per continent (see Chapter 5 of R for Data Science for a more detailed overview of summarize()): # data summary by groups world_continents = world %&gt;% group_by(continent) %&gt;% summarize(pop = sum(pop, na.rm = TRUE), country_n = n()) world_continents #&gt; Simple feature collection with 8 features and 3 fields #&gt; geometry type: GEOMETRY #&gt; dimension: XY #&gt; bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; # A tibble: 8 x 4 #&gt; continent pop country_n geom #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;simple_feature&gt; #&gt; 1 Africa 1.15e+09 51 &lt;MULTIPOLYGON...&gt; #&gt; 2 Antarctica 0.00e+00 1 &lt;MULTIPOLYGON...&gt; #&gt; 3 Asia 4.31e+09 47 &lt;MULTIPOLYGON...&gt; #&gt; 4 Europe 7.39e+08 39 &lt;MULTIPOLYGON...&gt; #&gt; # ... with 4 more rows sf objects are well-integrated with the tidyverse, as illustrated by the fact that the aggregated objects preserve the geometry of the original world object. This means that summaries of the world’s continents can be plotted spatially, as illustrated below, which generates a plot of population by continent (note that borders between countries have largely been removed): plot(world_continents[&quot;pop&quot;]) Figure 3.1: Geographic representation of attribute aggregation by continent: total population by continent generated by summarize(). The same result can be obtained using R’s base aggregrate() function. This has a slightly different syntax, requiring the grouping variable as a list: ag_var = list(world$continent) world_continents2 = aggregate(world[&quot;pop&quot;], by = ag_var, FUN = sum, na.rm = TRUE) % --> % --> 3.4 Attribute data joining Combining data from different sources is one of the most common task in data preparation. It could be done using joins - methods created to work with a pair of tables. The dplyr package has a set of verbs to easily connect data.frames - left_join(), right_join(), inner_join(), full_join, semi_join() and anti_join(). They are thoroughly explained in the Relational data chapter in the book R for Data Science (Grolemund and Wickham 2016). Working with spatial data, however, usually involves a connection between spatial data (sf objects) and tables (data.frame objects). Fortunately, the sf package has all of the dplyr join functions adapted to work with sf objects. The only important difference between combining two data.frames and combining sf with data.frame is a geom column. Therefore, the result of data joins could be either an sf or data.frame object. The easiest way to understand the concept of joins is to use a smaller datasets. We will use an sf object north_america with country codes (iso_a2), names and geometries, as well as data.frame object wb_north_america containing information about urban population and unemployment for three countries. It is important to add that the first object has data about Canada, Greenland and United States and the second one has data about Canada, Mexico and United States: north_america = world %&gt;% filter(subregion == &quot;Northern America&quot;) %&gt;% select(iso_a2, name_long) north_america #&gt; Simple feature collection with 3 features and 2 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -171.7911 ymin: 18.91619 xmax: -12.20855 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long geom #&gt; 1 CA Canada MULTIPOLYGON(((-63.6645 46.... #&gt; 2 GL Greenland MULTIPOLYGON(((-46.76379 82... #&gt; 3 US United States MULTIPOLYGON(((-155.54211 1... plot(north_america[0]) wb_north_america = worldbank_df %&gt;% filter(name %in% c(&quot;Canada&quot;, &quot;Mexico&quot;, &quot;United States&quot;)) %&gt;% select(name, iso_a2, urban_pop, unemploy = unemployment) wb_north_america #&gt; name iso_a2 urban_pop unemploy #&gt; 1 Canada CA 29022137 6.91 #&gt; 2 Mexico MX 99018446 5.25 #&gt; 3 United States US 259740511 6.17 In this book, we focus on spatial data. Most of the following examples will have a sf object as the first argument and a data.frame object as the second argument. A new sf object will be a result of these joins. However, the reverse order is also possible and will result in a data.frame object. This is mostly beyond the scope of this book, but we encourage you to try it. 3.4.1 Left joins Left join is the most often used type of joins. The left_join() returns all observations from the left object (north_america) and the matched observations from the right object (wb_north_america). In cases, like Greenland, when we don’t have a data in the right object, NA values will be introduced. To connect two object we need to specify a key. This is a variable (or variables) that uniquely identifies each observation (row). The argument by is used to state which variable is the key. In simple cases, a single, unique variable exist in both objects, for example iso_a2 column: left_join1 = north_america %&gt;% left_join(wb_north_america, by = &quot;iso_a2&quot;) left_join1 #&gt; Simple feature collection with 3 features and 5 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -171.7911 ymin: 18.91619 xmax: -12.20855 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long name urban_pop unemploy #&gt; 1 CA Canada Canada 29022137 6.91 #&gt; 2 GL Greenland &lt;NA&gt; NA NA #&gt; 3 US United States United States 259740511 6.17 #&gt; geom #&gt; 1 MULTIPOLYGON(((-63.6645 46.... #&gt; 2 MULTIPOLYGON(((-46.76379 82... #&gt; 3 MULTIPOLYGON(((-155.54211 1... It is also possible to join objects by different variables. Both of the datasets have variables with names of countries, but they are named differently. The north_america has a name_long column and the wb_north_america has a name column. In these cases, we can use a named vector to specify the connection, e.g. c(&quot;name_long&quot; = &quot;name&quot;): left_join2 = north_america %&gt;% left_join(wb_north_america, by = c(&quot;name_long&quot; = &quot;name&quot;)) left_join2 #&gt; Simple feature collection with 3 features and 5 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -171.7911 ymin: 18.91619 xmax: -12.20855 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2.x name_long iso_a2.y urban_pop unemploy #&gt; 1 CA Canada CA 29022137 6.91 #&gt; 2 GL Greenland &lt;NA&gt; NA NA #&gt; 3 US United States US 259740511 6.17 #&gt; geom #&gt; 1 MULTIPOLYGON(((-63.6645 46.... #&gt; 2 MULTIPOLYGON(((-46.76379 82... #&gt; 3 MULTIPOLYGON(((-155.54211 1... The new object left_join2, however is still not perfectly connected as it has two duplicated variables - iso_a2.x and iso_a2.y. To solve this problem we should specify all the keys: left_join3 = north_america %&gt;% left_join(wb_north_america, by = c(&quot;iso_a2&quot;, &quot;name_long&quot; = &quot;name&quot;)) left_join3 #&gt; Simple feature collection with 3 features and 4 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -171.7911 ymin: 18.91619 xmax: -12.20855 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long urban_pop unemploy geom #&gt; 1 CA Canada 29022137 6.91 MULTIPOLYGON(((-63.6645 46.... #&gt; 2 GL Greenland NA NA MULTIPOLYGON(((-46.76379 82... #&gt; 3 US United States 259740511 6.17 MULTIPOLYGON(((-155.54211 1... It is also possible to use our objects in the reverse order, where a data.frame object is the first argument and a sf object is the second argument. This would always result in a new data.frame object. For example, left_join() would create a new data.frame with a geom column: # keeps the geom column, but drops the sf class left_join4 = wb_north_america %&gt;% left_join(north_america, by = c(&quot;iso_a2&quot;)) left_join4 #&gt; name iso_a2 urban_pop unemploy name_long #&gt; 1 Canada CA 29022137 6.91 Canada #&gt; 2 Mexico MX 99018446 5.25 &lt;NA&gt; #&gt; 3 United States US 259740511 6.17 United States #&gt; geom #&gt; 1 MULTIPOLYGON(((-63.6645 46.... #&gt; 2 NULL #&gt; 3 MULTIPOLYGON(((-155.54211 1... class(left_join4) #&gt; [1] &quot;data.frame&quot; left_join4 has only one class - data.frame, however it is possible to add spatial sf class using the st_as_sf() function: left_join4_sf = st_as_sf(left_join4) left_join4_sf #&gt; Simple feature collection with 3 features and 5 fields (with 1 geometry empty) #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -171.7911 ymin: 18.91619 xmax: -12.20855 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; name iso_a2 urban_pop unemploy name_long #&gt; 1 Canada CA 29022137 6.91 Canada #&gt; 2 Mexico MX 99018446 5.25 &lt;NA&gt; #&gt; 3 United States US 259740511 6.17 United States #&gt; geom #&gt; 1 MULTIPOLYGON(((-63.6645 46.... #&gt; 2 MULTIPOLYGON() #&gt; 3 MULTIPOLYGON(((-155.54211 1... class(left_join4_sf) #&gt; [1] &quot;sf&quot; &quot;data.frame&quot; On the other hand, it is also possible to remove the geom column using base R functions or dplyr: # base R left_join4_df = subset(left_join4, select = -geom) # or dplyr left_join4_df = left_join4 %&gt;% select(-geom) left_join4_df #&gt; name iso_a2 urban_pop unemploy name_long #&gt; 1 Canada CA 29022137 6.91 Canada #&gt; 2 Mexico MX 99018446 5.25 &lt;NA&gt; #&gt; 3 United States US 259740511 6.17 United States class(left_join4_df) #&gt; [1] &quot;data.frame&quot; 3.4.2 Right joins right_join() keeps all observations from the second object (wb_north_america in this case) but preserves the sf class from the left object (north_america). right_join1 = north_america %&gt;% right_join(wb_north_america, by = c(&quot;iso_a2&quot;, &quot;name_long&quot; = &quot;name&quot;)) right_join1 #&gt; Simple feature collection with 3 features and 4 fields (with 1 geometry empty) #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -171.7911 ymin: 18.91619 xmax: -52.6481 ymax: 83.23324 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long urban_pop unemploy geom #&gt; 1 CA Canada 29022137 6.91 MULTIPOLYGON(((-63.6645 46.... #&gt; 2 MX Mexico 99018446 5.25 MULTIPOLYGON() #&gt; 3 US United States 259740511 6.17 MULTIPOLYGON(((-155.54211 1... The output shows that the result, right_join1, has information about Mexico, but drops information about Greenland. Furthermore, our right object, as a data.frame, doesn’t have a geometry representation of Mexico. As a result, the right_join1 object contains only non-spatial data of Mexico. It could be easily illustrated using the plot function: plot(right_join1[0]) # Canada and United States only 3.4.3 Inner joins The inner_join() keeps only observations from the left object (north_america) where there are matching observations in the right object (wb_north_america). Additionally, all columns from the left and right object are kept: inner_join1 = north_america %&gt;% inner_join(wb_north_america, by = c(&quot;iso_a2&quot;, &quot;name_long&quot; = &quot;name&quot;)) inner_join1 #&gt; Simple feature collection with 2 features and 4 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -171.7911 ymin: 18.91619 xmax: -52.6481 ymax: 83.23324 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long urban_pop unemploy geom #&gt; 1 CA Canada 29022137 6.91 MULTIPOLYGON(((-63.6645 46.... #&gt; 2 US United States 259740511 6.17 MULTIPOLYGON(((-155.54211 1... 3.4.4 Semi joins The semi_join() is very similar to the inner_join(). It also keeps only observations from the left object (north_america) where there are matching observations in the right object, but keeping just columns from the left one: semi_join1 = north_america %&gt;% semi_join(wb_north_america, by = &quot;iso_a2&quot;) semi_join1 #&gt; Simple feature collection with 2 features and 2 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -171.7911 ymin: 18.91619 xmax: -52.6481 ymax: 83.23324 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long geom #&gt; 1 CA Canada MULTIPOLYGON(((-63.6645 46.... #&gt; 2 US United States MULTIPOLYGON(((-155.54211 1... 3.4.5 Anti joins The anti_join() returns all rows from the left object that are not matching observations in the right object. Only columns from the right object are kept: anti_join1 = north_america %&gt;% anti_join(wb_north_america, by = &quot;iso_a2&quot;) anti_join1 #&gt; Simple feature collection with 1 feature and 2 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -73.297 ymin: 60.03676 xmax: -12.20855 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long geom #&gt; 1 GL Greenland MULTIPOLYGON(((-46.76379 82... plot(anti_join1[0]) 3.4.6 Full joins The full_join() returns all rows and all columns from both the left and right object. It also puts NA in cases where there are not matching values and returns an empty geometry for cases that only exist in the right object: full_join1 = north_america %&gt;% full_join(wb_north_america, by = &quot;iso_a2&quot;) full_join1 #&gt; Simple feature collection with 4 features and 5 fields (with 1 geometry empty) #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -171.7911 ymin: 18.91619 xmax: -12.20855 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs #&gt; iso_a2 name_long name urban_pop unemploy #&gt; 1 CA Canada Canada 29022137 6.91 #&gt; 2 GL Greenland &lt;NA&gt; NA NA #&gt; 3 US United States United States 259740511 6.17 #&gt; 4 MX &lt;NA&gt; Mexico 99018446 5.25 #&gt; geom #&gt; 1 MULTIPOLYGON(((-63.6645 46.... #&gt; 2 MULTIPOLYGON(((-46.76379 82... #&gt; 3 MULTIPOLYGON(((-155.54211 1... #&gt; 4 MULTIPOLYGON() 3.5 Attribute data creation It is often the case when a new column needs to be created based on existing columns. For example, we want to calculate population density for each country. We need to divide a pop column (population) by a area_km2 column (unit area in square km). It could be done this way in base R: data(&quot;world&quot;) world_new = world # do not overwrite our original data world_new$pop_dens = world_new$pop / world_new$area_km2 Alternatively, we can use one of dplyr functions - mutate() or transmute(). mutate() adds new columns at second-to-last position in the sf object (the last one is reserved for the geometry): world %&gt;% mutate(pop_dens = pop / area_km2) The difference between mutate() and transmute() is that the latter do not preserve existing columns: world %&gt;% transmute(pop_dens = pop / area_km2) Existing columns could be also paste together using unite(). For example, we want to stick together continent and region_un columns into a new con_reg column. We could specify a separator to use between values and if input columns should be removed: world_unite = world %&gt;% unite(con_reg, continent:region_un, sep = &quot;:&quot;, remove = TRUE) The separate() function is the complement of the unite() function. Its role is to split one column into multiple columns using either a regular expression or character position. world_separate = world_unite %&gt;% separate(con_reg, c(&quot;continent&quot;, &quot;region_un&quot;), sep = &quot;:&quot;) Two helper functions, rename() and set_names can be used to change columns names. The first one, rename() replace an old name with a new one. For example, we want to change a name of column from name_long to name: world %&gt;% rename(name = name_long) set_names can be used to change names of many columns. In this function, we do not need to provide old names: new_names = c(&quot;ISO_A2&quot;, &quot;Name&quot;, &quot;Continent&quot;, &quot;Region&quot;, &quot;Subregion&quot;, &quot;Country_type&quot;, &quot;Area_in_km2&quot;, &quot;Population&quot;, &quot;Life_Expectancy&quot;, &quot;GDP_per_capita&quot;, &quot;geom&quot;) world %&gt;% set_names(new_names) 3.6 Removing spatial information It is important to note that the attribute data operations preserve the geometry of the simple features. As mentioned at the outset of the chapter, however, it can be useful to remove the geometry. In the case of the world dataset we’ve been using, this can be done using st_set_geometry()16. world_data = world %&gt;% st_set_geometry(NULL) class(world_data) #&gt; [1] &quot;data.frame&quot; 3.7 Exercises For these exercises we’ll use the us_states and us_states_df datasets from the spData package: library(spData) data(&quot;us_states&quot;) data(&quot;us_states_df&quot;) us_states is a spatial object (of class sf), containing geometry and a few attributes (including name, region, area, and population) of states within the contiguous United States. us_states_df is a data frame (of class data.frame) containing the name and additional variables (including median income and poverty level, for years 2010 and 2015) of US states, including Alaska, Hawaii and Puerto Rico. The data comes from the US Census Bureau, and is documented in ?us_states and ?us_states_df. Create a new object called us_states_name that contains only the NAME column from the us_states object. What is the class of the new object? Select columns from the us_states object which contain population data. Obtain the same result using a different command (bonus: try to find 3 ways of obtaining the same result). Hint: try to use helper functions, such as contains or starts_with from dplyr (see ?contains). Find all states with the following characteristics (bonus find and plot them): Belongs to the Midwest region Belongs to the West region, has an area below 250,000 km2 and 20015 population greater than 5,000,000 residents (hint: you may need to use the function units::set_units() or as.numeric()) Belongs to the South region, had an area larger than 150,000 km2 or total population in 2015 larger than 7,000,000 residents What was the total population in 2015 in the us_states dataset? What was the minimum and maximum total population in 2015? How many states are there in each region? What was the minimum and maximum total population in 2015 in each region? What was the total population in 2015 in each region? Add variables from us_states_df to us_states and create a new object called us_states_stats. What function did you use and why? Which variable is the key in the both datasets? What is the class of a new object? us_states_df has two more variables than us_states. How you can find them? What was the population density in 2015 in each state? What was the population density in 2010 in each state? How much has population density changed between 2010 and 2015 in each state? Calculate the change in percentages and map them. Change the columns names in us_states to lowercase. (Hint: helper functions - tolower() and colnames() may help). Using us_states and us_states_df create a new object called us_states_sel. The new object should have only two variables - median_income_15 and geometry. Change the name of the median_income_15 column to Income. Calculate the change in median income between 2010 and 2015 for each state. Bonus: what was the minimum, average and maximum median income in 2015 for each region? What is the region with the largest increase of the median income? References "],
["spatial-data-operations.html", "4 Spatial data operations Prerequisites 4.1 Introduction 4.2 Spatial subsetting 4.3 Spatial data aggregation 4.4 Spatial data joining 4.5 Spatial data creation", " 4 Spatial data operations Prerequisites This chapter requires tidyverse, sf, units, and spData packages: library(sf) library(tidyverse) library(units) You must have loaded the world data from the spData package: library(spData) 4.1 Introduction 4.2 Spatial subsetting Spatial subsetting is the process of selecting only those features of a spatial object that in some way intersect with another spatial object. Note that ‘intersect’ in this context has a precise meaning: if y is used to subset features in a ‘target’ object of x, any features in x that touch, overlap or are within features in y will be selected. Intersect is the default operation for spatial subsetting but others can be used using the op = argument.17 There are 9 well-defined operations that can be used for spatial subsetting, covered in section 4.5.1. This may seem daunting but the good news is that you do not have to learn all of them separately: after you understand how to spatially subset objects that intersect another (via st_intersects()) it is easy to subset based on other types of spatial operation such as st_touches(), st_crosses() and st_within(). For this reason now we focus only pn one of the spatial subsetting operations. We use st_intersects() instead of any of the others not only because it the default when subsetting with [, but also st_intersects() is useful as a ‘catch all’ that identifies all types of spatial relations. In general terms, spatial subsetting is simply the spatial equivalent of attribute subsetting. However, to do spatial subsetting two spatial objects are needed the spatial relation between which is to be established. As with attribute subsetting, spatial subsetting is a binary operation: an object is either selected or not. As in section 3.2, we start with base methods before describing how to do it in the tidyverse. Attribute subsetting in base R is done with the [ operator and passing into the square brackets a vector of class integer (whole numbers) or logical (a vector of TRUEs and FALSEs). This means world[1:6,] subsets the first 6 countries of the world and that world[world$area_km2 &lt; 10000,] returns the subset of countries that have a small surface area. For this chapter we will use countries in Africa, which can be generated using this method as follows:18 africa_wgs = world[world$subregion == &quot;Western Africa&quot;, ] To further set-up the input data, we will reproject the data to the coordinate reference system (CRS) 32630 (it’s EPSG code, explained in Chapter 6): africa = st_transform(africa_wgs, crs = 32630) Spatial subsetting in base R use the same method as attribute subsetting, except another spatial object is placed inside the square brackets in the place of an integer or logical vector. This is a concise and consistent syntax, as shown in the next code chunk. Let’s test it with a hypothetical scenario: we want to subset all countries within 20 degrees of the point where the equator (where latitude = 0 degrees) intersects the prime meridian (longitude = 0 degrees), as illustrated in Figure 4.1. The subsetting object is created below. Note that this must have the same CRS as the target object (set with the crs argument): center = st_sf(st_sfc(st_point(c(0, 0)), crs = 4326)) buff = st_buffer(x = center, dist = 20) buff = st_transform(buff, 32630) Figure 4.1: Hypothetical subsetting scenario: select all countries which intersect with a circle of 20 degrees in radius around planet Earth. Figure created with the globe package. The data to be subset, or ‘target layer’, is the africa created above, which has a projected CRS (32630). Now that the input data is set-up, the spatial subsetting operation is a single, concise command: africa_buff = africa[buff,] Note that the command emits a message: about assuming planar coordinates. This is because spatial operations (especially distance and area calculations) cannot be assumed to be accurate in a geographic (longitude/latitude) CRS. In this case there is a clear justification: the data is close to the equator where there is least distortion caused by the curvature of the earth, and the example illustrates the method, which would more usually be used on pojected (‘planar’) data. In any case, the spatial subsetting clearly worked. As illustrated by Figure 4.2, only countries which spatially intersect with the giant circle are returned: plot(africa_buff[&quot;pop&quot;]) plot(buff, add = TRUE) Figure 4.2: Subset of the africa data selected based on their intersection with a circle 20 degrees in radius with a center point at 0 degrees longitude and 0 degrees latitude. Note that countries that only just touch the giant circle are selected such as the large country at the north of plot (Algeria). st_relates() includes countries that only touch (but are not within or overlapping with) the selection object. Other spatial subsetting operations such as st_within() are more conservative, as shown in section 4.5.1. Before we progress to explore the differences between different spatial subsetting operations, it is worth seeing alternative ways to acheive the same result, to deepen understanding of what is going on ‘under the hood’ (vital for developing advanced geocomputation applications). The second way to reproduce the subsetting operation illustrated in Figure 4.2 simply involves expanding the operation over 2 lines: sel_buff = st_intersects(x = africa, y = buff, sparse = FALSE) africa_buff2 = africa[sel_buff,] The third way is essentially the same as the second, but uses the filter() function introduced in section 3.2, forming the foundations of a ‘tidy’ spatial data analysis workflow. If you already use dplyr for data manipulation, this way should seem familiar: africa_buff3 = africa %&gt;% filter(st_intersects(x = ., y = buff, sparse = FALSE)) How can we be sure that the results obtained through the 4 subsetting operations demonstrated above? We can test them as follows: identical(x = africa_buff, y = africa_buff2) #&gt; [1] TRUE identical(x = africa_buff, y = africa_buff3) #&gt; [1] FALSE The reason that the third spatially subset object (africa_buff3) is not identical is that dplyr changes the row names: head(row.names(africa_buff)) #&gt; [1] &quot;14&quot; &quot;15&quot; &quot;32&quot; &quot;60&quot; &quot;61&quot; &quot;62&quot; head(row.names(africa_buff3)) #&gt; [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; If the row names are re-set, the objects become identical: attr(africa_buff3, &quot;row.names&quot;) = attr(x = africa_buff, &quot;row.names&quot;) identical(africa_buff, africa_buff3) #&gt; [1] TRUE Note This discarding of row names is not something that is specific to spatial data:19 row.names(africa[africa$subregion == &quot;Northern Europe&quot;,]) #&gt; character(0) row.names(filter(africa, subregion == &quot;Northern Europe&quot;)) #&gt; character(0) 4.3 Spatial data aggregation Like attribute data aggregation, covered in section 3.3, spatial data aggregation is a way of condensing data. Aggregated data show some statistic about a variable (typically mean average or total) in relation to some kind of grouping variable. For attribute data aggregation the grouping variable is another variable, typically one with few unique values relative to the number of rows. The continent variable in the world dataset is a good example: there are only 8 unique continents but 177 countries. In section 3.3 the aggregation process condensed the world dataset down into only 8 rows and an aggregated pop variable representing the total population per continent (see Figure 3.1). Spatial data aggregation is the same conceptually but uses a spatial grouping object: the output is the same, in terms of number of rows/features and geometry, as the grouping object, but with new variables corresponding to the input dataset. As with spatial subsetting, spatial aggregation operations work by extending existing functions. Since mid-2017 (with the release of sf 0.5-3) the base R function aggregate() works with a spatial object as a grouping variable. Building on the example presented the previous section (4.2), we demonstrate this by aggregating the population of countries that intersect with the buffer represented by the circular buff object created in the previous section. buff_agg = aggregate(x = africa[&quot;pop&quot;], by = buff, FUN = sum) The result, buff_agg, is a spatial object with the same geometry as by (the circular buffer in this case) but with an additional variable, pop reporting summary statistics for all features in x that intersect with by (the total population of the countries that touch the buffer in this case). Plotting the result (with plot(buff_agg)) shows that the operation does not really make sense: Figure 4.3 shows a population of over half a billion people mostly located in a giant circle floating off the west coast of Africa! Figure 4.3: Result of spatial aggregation showing the total population of countries that intersect with a large circle whose center lies at 0 degrees longitude and latituge The results of the spatial aggregation exercise presented in Figure 4.3 are unrealistic for three reasons: People do not live in the sea (the geometry of the aggregating object is not appropriate for the geometry target object). This method would ‘double count’ countries whose borders cross aggregating polygons when multiple, spatially contiguous, features are used as the aggregating object. It is wrong to assume that all the people living in countries that touch the buffer reside within it (the default spatial operator st_intersects() is too ‘greedy’). The most extreme example of this is Algeria, the most northerly country selected: the spatial aggregation operation assumes that all 39 million Algerian citizens reside in the tiny southerly tip that is within the circular buffer. A number of methods can be used to overcome these issues, which result in unrealistically high population attributed to the circular buffer illustrated in Figure 4.3. The simplest of these is to convert the country polygons into points representing their geographic centroids before aggregation. This would ensure that any spatially contiguous aggregating object covering the target object (the Earth in this case) would result in the same total: there would be no double counting. The estimated total population residing within the study area would be more realistic if geographic centroids were used. (The centroid of Algeria, for example, is far outside the aggregating buffer.) Except in cases where the number of target features per aggregating feature is very large, or where the aggregating object is spatially congruent with the target (covered in section 4.3.1), using centroids can also lead to errors due to boundary effects: imagine a buffer that covers a large area but contains no centroids. These issues can be tackled when aggregating areal target data with areal interpolation. 4.3.1 Spatial congruence and areal interpolation Spatial congruence is an important concept when interpreting the results of spatial aggregation and other operations. An aggregating object object (which we will refer to as y, representing the buffer object in the previous section) is congruent with the target object x, corresponding to the first argument of aggregate() in the previous section, if the two objects have shared borders: no feature in x overlaps one or more features in y. Incongruent objects, by contrast do not share common borders (Qiu, Zhang, and Zhou 2012). This is the case illustrated in Figure 4.3, where the borders of the countries in the ‘target’ object bear no relation to, and frequently cross, the outline of the aggregating buffer. This is problematic if one wants to convert from type of areal unit that is incongruent with another, as illustrated in Figure 4.4. Areal interpolation resolves this problem. A number of methods have been developed to for the task, all of which start from the following constraint, succinctly described in a classic paper on the subject (Tobler 1979): “that the original [target] data arrive packaged in discrete collection regions.” Figure 4.4: Illustration of congruent (left) and incongruent (right) areal units. The simplest useful method for spatial interpolation is area weighted spatial interpolation. This is implemented in st_interpolate_aw(), as demonstrated below: buff_agg_aw = st_interpolate_aw(x = africa[&quot;pop&quot;], to = buff, extensive = TRUE) #&gt; Warning in st_interpolate_aw(x = africa[&quot;pop&quot;], to = buff, extensive = #&gt; TRUE): st_interpolate_aw assumes attributes are constant over areas of x 4.4 Spatial data joining save as GPKG? --> 4.5 Spatial data creation # add a new column africa$area = set_units(st_area(africa), value = km^2) africa$pop_density = africa$pop / africa$area # OR africa = africa %&gt;% mutate(area = set_units(st_area(.), value = km^2)) %&gt;% mutate(pop_density = pop / area) Note that this has created a attributes for the area and population density variables: attributes(africa$area) #&gt; $units #&gt; $numerator #&gt; [1] &quot;km&quot; &quot;km&quot; #&gt; #&gt; $denominator #&gt; character(0) #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;symbolic_units&quot; #&gt; #&gt; $class #&gt; [1] &quot;units&quot; attributes(africa$pop_density) #&gt; $units #&gt; $numerator #&gt; character(0) #&gt; #&gt; $denominator #&gt; [1] &quot;km&quot; &quot;km&quot; #&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;symbolic_units&quot; #&gt; #&gt; $class #&gt; [1] &quot;units&quot; These can be set to NULL as follows: attributes(africa$area) = NULL attributes(africa$pop_density) = NULL 4.5.1 Topological relations (2) --> a1 = st_polygon(list(rbind(c(-1, -1), c(1, -1), c(1, 1), c(-1, -1)))) a2 = st_polygon(list(rbind(c(2, 0), c(2, 2), c(3, 2), c(3, 0), c(2, 0)))) a = st_sfc(a1, a2) b1 = a1 * 0.5 b2 = a2 * 0.4 + c(1, 0.5) b = st_sfc(b1, b2) l1 = st_linestring(x = matrix(c(0, 3, -1, 1), , 2)) l2 = st_linestring(x = matrix(c(-1, -1, -0.5, 1), , 2)) l = st_sfc(l1, l2) p = st_multipoint(x = matrix(c(0.5, 1, -1, 0, 1, 0.5), , 2)) plot(a, border = &quot;red&quot;, axes = TRUE) plot(b, border = &quot;green&quot;, add = TRUE) plot(l, add = TRUE) plot(p, add = TRUE) Equals: st_equals(a, b, sparse = FALSE) Contains: st_contains(a, b, sparse = FALSE) st_contains_properly(a, b, sparse = FALSE) Covers: st_covers(a, b, sparse = FALSE) st_covered_by(a, b, sparse = FALSE) Within: st_within(a, b, sparse = FALSE) Overlaps: st_overlaps(a, b, sparse = FALSE) Intersects: st_intersects(a, b, sparse = FALSE) Disjoint: st_disjoint(a, b, sparse = FALSE) Touches: st_touches(a, b, sparse = FALSE) Crosses: st_crosses(a, b, sparse = FALSE) DE9-IM - https://en.wikipedia.org/wiki/DE-9IM st_relate(a, b, sparse = FALSE) 4.5.2 Distance relations st_distance(a, b) 4.5.3 Spatial clipping Spatial clipping is a form of spatial subsetting that involves changes to the geometry columns of at least some of the affected features. Clipping can only apply to features more complex than points: lines, polygons and their ‘multi’ equivalents. To illustrate the concept we will start with a simple example: two overlapping circles with a centrepoint 1 unit away from each other and radius of 1: b = st_sfc(st_point(c(0, 1)), st_point(c(1, 1))) # create 2 points b = st_buffer(b, dist = 1) # convert points to circles l = c(&quot;x&quot;, &quot;y&quot;) plot(b) text(x = c(-0.5, 1.5), y = 1, labels = l) # add text Figure 4.5: Overlapping circles. Imagine you want to select not one circle or the other, but the space covered by both x and y. This can be done using the function st_intersection(), illustrated using objects named x and y which represent the left and right-hand circles: x = b[1] y = b[2] x_and_y = st_intersection(x, y) plot(b) plot(x_and_y, col = &quot;lightgrey&quot;, add = TRUE) # color intersecting area The subsequent code chunk demonstrate how this works for all combinations of the ‘venn’ diagram representing x and y, inspired by Figure 5.1 of the book R for Data Science (Grolemund and Wickham 2016). Figure 4.6: Spatial equivalents of logical operators To illustrate the relationship between subsetting and clipping spatial data, we will subset points that cover the bounding box of the circles x and y in Figure 4.6. Some points will be inside just one circle, some will be inside both and some will be inside neither. To generate the points will use a function not yet covered in this book, st_sample(). There are two different ways to subset points that fit into combinations of the circles: via clipping and logical operators. But first we must generate some points. We will use the simple random sampling strategy to sample from a box representing the extent of x and y, using the code below to generate the situation plotted in Figure 4.7: bb = st_bbox(st_union(x, y)) pmat = matrix(c(bb[c(1, 2, 3, 2, 3, 4, 1, 4, 1, 2)]), ncol = 2, byrow = TRUE) box = st_polygon(list(pmat)) set.seed(2017) p = st_sample(x = box, size = 10) plot(box) plot(x, add = T) plot(y, add = T) plot(p, add = T) text(x = c(-0.5, 1.5), y = 1, labels = l) Figure 4.7: Randomly distributed points within the bounding box enclosing circles x and y. 4.5.4 Exercises Write code that subsets points that are contained within x and y (illustrated by the plot in the 2nd row and the 1st column in Figure 4.6). Create a randomly located point with the command st_point() (refer back to section 2.1.4.2 to see how to create spatial data ‘from scratch’). Write code that uses functions aggregate() and st_buffer() to answers the following question: What proportion of the world’s population lives in countries that intersect a circle with a 10 degree radius of the intersection between the equator and the 9th meridian? (Advanced challenge: find the point with the highest number of people within a 10 degree radius.) center9 = st_sf(st_sfc(st_point(c(-9, 0)), crs = 4326)) buff9 = st_buffer(center9, dist = 10) #&gt; Warning in st_buffer.sfc(st_geometry(x), dist, nQuadSegs): st_buffer does #&gt; not correctly buffer longitude/latitude data, dist needs to be in decimal #&gt; degrees. agg9 = aggregate(world[&quot;pop&quot;], buff9, FUN = sum) #&gt; although coordinates are longitude/latitude, it is assumed that they are planar agg9$pop / sum(world$pop, na.rm = TRUE) #&gt; [1] 0.00998 Assuming that people are evenly distributed across countries, estimate the population living within the circle created to answer the previous question. interp9 = st_interpolate_aw(x = world[&quot;pop&quot;], to = buff9, extensive = TRUE) #&gt; Warning in st_interpolate_aw(x = world[&quot;pop&quot;], to = buff9, extensive = #&gt; TRUE): st_interpolate_aw assumes attributes are constant over areas of x References "],
["read-write.html", "5 Geographic data I/O 5.1 Data Input (I) 5.2 Data output (O) 5.3 File formats 5.4 Visual outputs 5.5 Exercises", " 5 Geographic data I/O The previous chapters provided an overview of spatial data classes in R, with a focus on simple features. This chapter is about getting spatial data onto your computer and then, perhaps after processing it with techniques described in this book, back out to the world. We include a section (5.4) on visualization because outputting data in a human (not just computer) readable format enables non-programmers to benefit from your work. If your aim is to use geocomputation to improve the world, e.g. by encouraging evidence-based policies, this final stage is vital. I/O is short for “input/output” which means, in plain English, “reading and writing data”. We use the acronym instead of plain English not to confuse you or to make chapter names short, but because that’s the term used in computer science and it is useful to think of data import and export from a computing perspective.20 5.1 Data Input (I) To efficiently read data into R, it helps to have an understanding of what happens ‘under the hood’. Executing commands such as sf::st_read() (the main function we use for loading spatial data, from the sf package) or readr::read_csv() silently sets off a chain of events that loads objects. “Loading” in this context means loading the data into R or, more precisely, assigning objects to your workspace, stored in RAM accessible from the .GlobalEnv of your current R session. Spatial data comes in a wide variety of file formats, and sf is able to handle most of them via its st_read() command. Behind the scenes it uses GDAL, which supports the import of a very wide range of spatial data formats. The first argument of st_read() is file, which should be a text string or an object containing a single text string: library(sf) #&gt; Linking to GEOS 3.5.0, GDAL 2.1.0, proj.4 4.8.0 f = system.file(&quot;shapes/world.gpkg&quot;, package = &quot;spData&quot;) world = st_read(f) #&gt; Reading layer `wrld.gpkg&#39; from data source `/home/travis/R/Library/spData/shapes/world.gpkg&#39; using driver `GPKG&#39; #&gt; Simple feature collection with 177 features and 10 fields #&gt; geometry type: MULTIPOLYGON #&gt; dimension: XY #&gt; bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513 #&gt; epsg (SRID): 4326 #&gt; proj4string: +proj=longlat +datum=WGS84 +no_defs Tip: read_sf() and write_sf() can be used as easy-to-remember alternatives to st_read() and st_write(). Remember they hide information about the data source and overwrite existing data, though. A major advantage of sf is that it is fast. To demonstrate this, we will use a function to compare st_read with its sp equivalent, rgdal::readOGR: bench_read = function(file, n) { m = microbenchmark(times = n, rgdal::readOGR(f), st_read(f) ) mean(m$time[1:n]) / mean(m$time[(n + 1):(n * 2)]) } This function takes as arguments an input file (file) and a number of times to run each command (n) and returns how many times faster st_read() is than readOGR(). Let’s run the benchmark for the world.gpkg file represented by the object f: library(microbenchmark) read_world_gpkg = bench_read(file = f, n = 5) read_world_gpkg #&gt; [1] 2.28 The results demonstrate that sf was around 2 times faster than rgdal at reading-in the world countries shapefile. The relative performance of st_read() compared with other functions will vary depending on file format and the nature of the data. To illustrate this point, we performed the same operation on a geojson file and found a greater speed saving: f = system.file(&quot;shapes/cycle_hire_osm.geojson&quot;, package = &quot;spData&quot;) read_lnd_geojson = bench_read(file = f, n = 5) read_lnd_geojson #&gt; [1] 3.35 In this case sf was around 3 times faster than rgdal. To find out which data formats sf supports, run st_drivers(). Here, we show only the first two drivers: sf_drivers = st_drivers() head(sf_drivers, n = 2) #&gt; name long_name write copy is_raster is_vector #&gt; PCIDSK PCIDSK PCIDSK Database File TRUE FALSE TRUE TRUE #&gt; netCDF netCDF Network Common Data Format TRUE TRUE TRUE TRUE 5.2 Data output (O) The counterpart of st_read() is st_write(). It allows to write sf objects to a wide range of geographic vector file formats, including the most common ones such as .geojson, .shp and .gpkg. Based on the file name st_write() decides automatically which driver to use. How fast the writing process is depends also on the driver: system.time(st_write(world, &quot;world.geojson&quot;, quiet = TRUE)) #&gt; user system elapsed #&gt; 0.064 0.000 0.065 system.time(st_write(world, &quot;world.shp&quot;, quiet = TRUE)) #&gt; user system elapsed #&gt; 0.044 0.000 0.046 system.time(st_write(world, &quot;world.gpkg&quot;, quiet = TRUE)) #&gt; user system elapsed #&gt; 0.024 0.004 0.030 Note: if you try to write to the same data source again, the function will fail. This is demonstrated in the code below for a modified version of the world in which the population doubles in all countries (don’t worry about the dplyr code for now, this is covered in Chapter 3): world_mod = dplyr::mutate(world, pop = pop * 2) st_write(obj = world_mod, dsn = &quot;world.gpkg&quot;) ## GDAL Error 1: Layer world.gpkg already exists, CreateLayer failed. ## Use the layer creation option OVERWRITE=YES to replace it. The error message (only partly reproduced above) provides some information as to why the function failed. The GDAL Error 1 statement makes clear that the failure occurred at the GDAL level. Additionally, the suggestion to use OVERWRITE=YES provides a clue how to fix the problem. However, this is a GDAL option, and not a st_write() argument. Luckily, st_write provides a layer_options argument through which we can pass driver-dependent options: st_write(obj = world_mod, dsn = &quot;world.gpkg&quot;, layer_options = &quot;OVERWRITE=YES&quot;) Another solution is to use the st_write() argument delete_layer. Setting it to TRUE deletes already existing layers in the data source before the function attempts to write (note there is also a delete_dsn argument): st_write(obj = world_mod, dsn = &quot;world.gpkg&quot;, delete_layer = TRUE) You can achieve the same with write_sf() since it is equivalent to (technically an alias for) st_write(), except that its defaults for delete_layer and quiet is TRUE. This enables spatial data to be overwritten more concisely, and with less output going to screen: write_sf(obj = world_mod, dsn = &quot;world.gpkg&quot;) A blunter way to update file-based geographic data sources such as .gpkg files is to simply delete them. This is not generally recommended, as it will not work for multi-file data sources such as .shp files: file.remove(&quot;world.gpkg&quot;) 5.3 File formats 5.4 Visual outputs 5.5 Exercises Name three differences between write_sf() and the more well-known function st_write(). What are the default arguments of read_sf() and write_sf() that enable two of these differences? Concepts such as computational efficiency, hard disk space and ‘idempotence’ are useful when thinking about reading and writing geographic datasets, which can become large and difficult to handle. Loading/saving data is yet another way of saying the same thing.↩ "],
["references.html", "6 References", " 6 References "]
]
